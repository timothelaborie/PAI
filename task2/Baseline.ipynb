{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install blitz-bayesian-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTFgJMBXkyy3",
        "outputId": "08c8246a-f5eb-4ffc-baf6-d4ce0f4328e8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: blitz-bayesian-pytorch in /usr/local/lib/python3.7/dist-packages (0.2.8)\n",
            "Requirement already satisfied: pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from blitz-bayesian-pytorch) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from blitz-bayesian-pytorch) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from blitz-bayesian-pytorch) (1.0.2)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from blitz-bayesian-pytorch) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from blitz-bayesian-pytorch) (0.13.1+cu113)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->blitz-bayesian-pytorch) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->blitz-bayesian-pytorch) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->blitz-bayesian-pytorch) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->blitz-bayesian-pytorch) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->blitz-bayesian-pytorch) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->blitz-bayesian-pytorch) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->blitz-bayesian-pytorch) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->blitz-bayesian-pytorch) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.5.0->blitz-bayesian-pytorch) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from blitz.modules import BayesianLinear, BayesianConv2d\n",
        "from blitz.losses import kl_divergence_from_nn\n",
        "from blitz.utils import variational_estimator"
      ],
      "metadata": {
        "id": "uut6XTTMk4PT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import typing\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import trange\n",
        "import tqdm\n",
        "from torch.distributions import Poisson\n",
        "from collections import deque\n",
        "import copy\n",
        "from enum import Enum\n",
        "# @title\n",
        "\n",
        "import abc\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim import SGD\n"
      ],
      "metadata": {
        "id": "V245zOWzjGQ6"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Reliability_diagram_1. Set `EXTENDED_EVALUATION` to `True` in order to visualize your predictions.\n",
        "EXTENDED_EVALUATION = False\n",
        "\n",
        "class Approach(Enum):\n",
        "    Dummy_Trainer = 0\n",
        "    MCDropout = 1\n",
        "    Ensemble = 2\n",
        "    Backprop = 3\n",
        "    SGLD = 4\n",
        "    SelfMade = 5 \n",
        "\n",
        "\n",
        "def run_solution(dataset_train: torch.utils.data.Dataset, data_dir: str = os.curdir, output_dir: str = '/results/'):\n",
        "    \"\"\"\n",
        "    Run your task 2 solution.\n",
        "    This method should train your model, evaluate it, and return the trained model at the end.\n",
        "    Make sure to preserve the method signature and to return your trained model,\n",
        "    else the checker will fail!\n",
        "\n",
        "    :param dataset_train: Training dataset\n",
        "    :param data_dir: Directory containing the datasets\n",
        "    :return: Your trained model\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Combined model_1: Choose if you want to combine with multiple methods or not\n",
        "    combined_model = False\n",
        "\n",
        "    if not combined_model:\n",
        "        \n",
        "        # TODO General_1: Choose your approach here\n",
        "        approach = Approach.Dummy_Trainer\n",
        "\n",
        "        # if approach == Approach.Dummy_Trainer:\n",
        "        #     trainer = DummyTrainer(dataset_train=dataset_train)\n",
        "        # if approach == Approach.MCDropout:\n",
        "        #     trainer = DropoutTrainer(dataset_train=dataset_train)\n",
        "        # if approach == Approach.Ensemble:\n",
        "        #     trainer = EnsembleTrainer(dataset_train=dataset_train)\n",
        "        # if approach == Approach.Backprop:\n",
        "        #     trainer = BackpropTrainer(dataset_train=dataset_train)\n",
        "        # if approach == Approach.SGLD:\n",
        "        #     trainer = SGLDTrainer(dataset_train=dataset_train)\n",
        "        # if approach == Approach.SelfMade:\n",
        "        #     trainer = SelfTrainer(dataset_train=dataset_train)\n",
        "        # trainer = BayesTrainer(dataset_train=dataset_train)\n",
        "        trainer = BayesTrainerD(dataset_train=dataset_train)\n",
        "        # Train the model\n",
        "        print('Training model', approach.name)\n",
        "        trainer.train()\n",
        "\n",
        "        # Predict using the trained model\n",
        "        print('Evaluating model on training data')\n",
        "        eval_loader = torch.utils.data.DataLoader(\n",
        "            dataset_train, batch_size=64, shuffle=False, drop_last=False\n",
        "        )\n",
        "        evaluate(trainer, eval_loader, data_dir, output_dir)\n",
        "\n",
        "        return trainer\n",
        "        \n",
        "    elif combined_model:\n",
        "        # using combined model\n",
        "\n",
        "        # TODO: Combined model_2: If you want to use combined methods \n",
        "        # you can set the trainers you want to use like below\n",
        "        trainer1 = DummyTrainer(dataset_train=dataset_train)\n",
        "        trainer2 = DummyTrainer(dataset_train=dataset_train)\n",
        "        trainer3 = DummyTrainer(dataset_train=dataset_train)\n",
        "        trainer_list = [trainer1,trainer2,trainer3]\n",
        "\n",
        "        # Train the combined model\n",
        "        for trainer_i in trainer_list:\n",
        "            trainer_i.train()\n",
        "\n",
        "        # Evaulate each of the combined models\n",
        "        print('Evaluating model on training data')\n",
        "        eval_loader = torch.utils.data.DataLoader(\n",
        "            dataset_train, batch_size=64, shuffle=False, drop_last=False\n",
        "        )\n",
        "        for trainer_i in trainer_list:\n",
        "            evaluate(trainer_i, eval_loader, data_dir, output_dir)\n",
        "\n",
        "    # IMPORTANT: return your combined model here!\n",
        "        return trainer_list"
      ],
      "metadata": {
        "id": "GBYY1Bivjzch"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_calibration_curve(predicted_probabilities: np.ndarray, labels: np.ndarray, num_bins=30)-> dict:\n",
        "    \"\"\"\n",
        "    Calculate ece and understand what is good calibration. This task is part of the \n",
        "    extended evaluation and not required for passing. \n",
        "    \"\"\"\n",
        "\n",
        "    num_samples, num_classes = predicted_probabilities.shape\n",
        "    predicted_classes = np.argmax(predicted_probabilities, axis=1) \n",
        "    confidences = predicted_probabilities[range(num_samples), predicted_classes]\n",
        "    bins = np.linspace(start=0,stop=1,num=num_bins+1)\n",
        "\n",
        "    bin_lowers = bins[:-1]\n",
        "    bin_uppers = bins[1:]\n",
        "    accuracies = predicted_classes == labels\n",
        "\n",
        "    calib_confidence = np.zeros(num_bins,dtype=np.float)\n",
        "    calib_accuracy = np.zeros(num_bins,dtype=np.float)\n",
        "    ratios = np.zeros(num_bins,dtype=np.float)\n",
        "\n",
        "    for bin_i, (bin_lower, bin_upper) in enumerate(zip(bin_lowers, bin_uppers)):\n",
        "        # TODO: Reliability_diagram_2. Calculate calibration confidence, accuracy in every bin\n",
        "        calib_confidence[bin_i] = None\n",
        "        calib_accuracy[bin_i] = None\n",
        "        ratios[bin_i] = None\n",
        "    \n",
        "    return {\"calib_confidence\": calib_confidence, \"calib_accuracy\": calib_accuracy, \"p\": ratios, \"bins\": bins}\n",
        "\n",
        "\n",
        "class Framework(object):\n",
        "    def __init__(self, dataset_train:torch.utils.data.Dataset, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Basic Framework for your bayesian neural network.\n",
        "        Other solutions like MC Dropout, Ensemble learning will based upon this.\n",
        "        \"\"\"\n",
        "        self.train_set = dataset_train\n",
        "        self.print_interval = 100 # number of batches until updated metrics are displayed during training\n",
        "\n",
        "    def train(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def predict(self, data_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict the class probabilities using your trained model.\n",
        "        This method should return an (num_samples, 10) NumPy float array\n",
        "        such that the second dimension sums up to 1 for each row.\n",
        "\n",
        "        :param data_loader: Data loader yielding the samples to predict on\n",
        "        :return: (num_samples, 10) NumPy float array where the second dimension sums up to 1 for each row\n",
        "        \"\"\"\n",
        "        probability_batches = []\n",
        "        \n",
        "        for batch_x, _ in tqdm.tqdm(data_loader):\n",
        "            current_probabilities = self.predict_probabilities(batch_x).detach().cpu().numpy()\n",
        "            probability_batches.append(current_probabilities)\n",
        "\n",
        "        output = np.concatenate(probability_batches, axis=0)\n",
        "        assert isinstance(output, np.ndarray)\n",
        "        assert output.ndim == 2 and output.shape[1] == 10\n",
        "        assert np.allclose(np.sum(output, axis=1), 1.0)\n",
        "        return output\n",
        "\n",
        "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        raise NotImplementedError()"
      ],
      "metadata": {
        "id": "4JTes7y0kK-X"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_predict(data_loader: torch.utils.data.DataLoader, models_list: list) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Predict the class probabilities using a combination of your trained model.\n",
        "    This method should return an (num_samples, 10) NumPy float array - the same as \n",
        "    predict()- such that the second dimension sums up to 1 for each row.\n",
        "    :param data_loader: Data loader yielding the samples to predict on\n",
        "    :return: (num_samples, 10) NumPy float array where the second dimension sums up to 1 for each row\n",
        "    \"\"\"\n",
        "\n",
        "    probability_batches = []\n",
        "\n",
        "    for batch_x, _ in tqdm.tqdm(data_loader):\n",
        "        # TODO: Combined model_3. Predict with your combined model\n",
        "        current_probabilities = None\n",
        "        probability_batches.append(current_probabilities)\n",
        "        \n",
        "    output = np.concatenate(probability_batches, axis=0)\n",
        "    assert isinstance(output, np.ndarray)\n",
        "    assert output.ndim == 2 and output.shape[1] == 10\n",
        "    assert np.allclose(np.sum(output, axis=1), 1.0)\n",
        "    return output"
      ],
      "metadata": {
        "id": "KwiLHyQekU8N"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyTrainer(Framework):\n",
        "    \"\"\"\n",
        "    Trainer implementing a simple feedforward neural network.\n",
        "    You can learn how to build your own trainer and use this model as a reference/baseline for \n",
        "    the calibration of a standard Neural Network. \n",
        "    \"\"\"\n",
        "    def __init__(self, dataset_train,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__(dataset_train, *args, **kwargs)\n",
        "\n",
        "        # Hyperparameters and general parameters\n",
        "        self.batch_size = 128\n",
        "        self.learning_rate = 1e-3\n",
        "        self.num_epochs = 100\n",
        "\n",
        "\n",
        "        self.network = MNISTNet(in_features=28*28,out_features=10)\n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            dataset_train, batch_size=self.batch_size, shuffle=True, drop_last=True\n",
        "            )\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate) \n",
        "        \n",
        "    def train(self):\n",
        "        self.network.train()\n",
        "        progress_bar = trange(self.num_epochs)\n",
        "        for _ in progress_bar:\n",
        "            for batch_idx, (batch_x, batch_y) in enumerate(self.train_loader):\n",
        "                # batch_x are of shape (batch_size, 784), batch_y are of shape (batch_size,)\n",
        "\n",
        "                self.network.zero_grad()\n",
        "\n",
        "                # Perform forward pass\n",
        "                current_logits = self.network(batch_x)\n",
        "\n",
        "                # Calculate the loss\n",
        "                # We use the negative log likelihood as the loss\n",
        "                # Combining nll_loss with a log_softmax is better for numeric stability\n",
        "                loss = F.nll_loss(F.log_softmax(current_logits, dim=1), batch_y, reduction='sum')\n",
        "\n",
        "                # Backpropagate to get the gradients\n",
        "                loss.backward()\n",
        "\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Update progress bar with accuracy occasionally\n",
        "                if batch_idx % self.print_interval == 0:\n",
        "                    current_logits = self.network(batch_x)\n",
        "\n",
        "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
        "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item())\n",
        "\n",
        "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # using the confidence as estimated probaility, \n",
        "        self.network.eval()\n",
        "        # assert x.shape[1] == 28 ** 2\n",
        "        with torch.no_grad():\n",
        "          estimated_probability = F.softmax(self.network(x), dim=1)\n",
        "          assert estimated_probability.shape == (x.shape[0], 10)\n",
        "        return estimated_probability"
      ],
      "metadata": {
        "id": "Gq7WyFlekYNc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@variational_estimator\n",
        "class BayesianCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = BayesianConv2d(1, 6, (5,5))\n",
        "        self.conv2 = BayesianConv2d(6, 16, (5,5))\n",
        "        self.fc1   = BayesianLinear(256, 120)\n",
        "        self.fc2   = BayesianLinear(120, 84)\n",
        "        self.fc3   = BayesianLinear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = F.relu(self.conv2(out))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "BbnF-xj7k9nE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@variational_estimator\n",
        "class BayesianCNND(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = BayesianConv2d(1, 6, (5,5))\n",
        "        self.conv2 = BayesianConv2d(6, 16, (5,5))\n",
        "        self.fc1   = BayesianLinear(256, 120)\n",
        "        self.fc2   = BayesianLinear(120, 84)\n",
        "        self.fc3   = BayesianLinear(84, 10)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.conv1(x))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = F.relu(self.conv2(out))\n",
        "        out = F.max_pool2d(out, 2)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = F.relu(self.dropout(self.fc1(out)))\n",
        "        out = F.relu(self.dropout(self.fc2(out)))\n",
        "        out = self.fc3(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "gegOiEdlD8Ps"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "6ZYWNvR0tvO0"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesTrainer(Framework):\n",
        "    \"\"\"\n",
        "    Trainer implementing a simple feedforward neural network.\n",
        "    You can learn how to build your own trainer and use this model as a reference/baseline for \n",
        "    the calibration of a standard Neural Network. \n",
        "    \"\"\"\n",
        "    def __init__(self, dataset_train,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__(dataset_train, *args, **kwargs)\n",
        "\n",
        "        # Hyperparameters and general parameters\n",
        "        self.batch_size = 64\n",
        "        self.learning_rate = 1e-3\n",
        "        self.num_epochs = 100\n",
        "\n",
        "\n",
        "        self.network = BayesianCNN().to(device)\n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            dataset_train, batch_size=self.batch_size, shuffle=True, drop_last=True\n",
        "            )\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate) \n",
        "        \n",
        "    def train(self):\n",
        "        self.network.train()\n",
        "        iteration = 0\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # classifier = BayesianCNN().to(device)\n",
        "        # optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "        # criterion = F.nll_loss\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        for epoch in trange(self.num_epochs):\n",
        "            for i, (datapoints, labels) in enumerate(self.train_loader):\n",
        "                datapoints = datapoints.unsqueeze(1).to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = self.network.sample_elbo(inputs=datapoints.to(device),\n",
        "                                  labels=labels.to(device),\n",
        "                                  criterion=criterion,\n",
        "                                  sample_nbr=3,\n",
        "                                  complexity_cost_weight=1/50000)\n",
        "                #print(loss)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                iteration += 1\n",
        "                if iteration%250==0:\n",
        "                    print(loss)\n",
        "                    correct = 0\n",
        "                    total = 0\n",
        "                    print('Evaluating model on training data')\n",
        "                    eval_loader = torch.utils.data.DataLoader(\n",
        "                        dataset_train, batch_size=64, shuffle=False, drop_last=False\n",
        "                    )\n",
        "                    evaluate(self, eval_loader, data_dir, output_dir)\n",
        "                    torch.save(self.network.state_dict(), f'bayesnn{iteration}.pth')\n",
        "        torch.save(self.network.state_dict(), 'bayesnn.pth')\n",
        "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # using the confidence as estimated probaility, \n",
        "        # print(f\"x shape: {x.shape}\")\n",
        "        self.network.eval()\n",
        "        # assert x.shape[1] == 28 ** 2\n",
        "        x = x.squeeze(0).unsqueeze(1)\n",
        "        estimated_probability = F.softmax(self.network(x), dim=1)\n",
        "        # assert estimated_probability.shape == (x.shape[0], 10)\n",
        "        return estimated_probability"
      ],
      "metadata": {
        "id": "TQPYU4nTtPsO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BayesTrainerD(Framework):\n",
        "    \"\"\"\n",
        "    Trainer implementing a simple feedforward neural network.\n",
        "    You can learn how to build your own trainer and use this model as a reference/baseline for \n",
        "    the calibration of a standard Neural Network. \n",
        "    \"\"\"\n",
        "    def __init__(self, dataset_train,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__(dataset_train, *args, **kwargs)\n",
        "\n",
        "        # Hyperparameters and general parameters\n",
        "        self.batch_size = 64\n",
        "        self.learning_rate = 0.0005\n",
        "        self.num_epochs = 100\n",
        "\n",
        "\n",
        "        self.network = BayesianCNND().to(device)\n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            dataset_train, batch_size=self.batch_size, shuffle=True, drop_last=True\n",
        "            )\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate) \n",
        "        \n",
        "    def train(self):\n",
        "        self.network.train()\n",
        "        iteration = 0\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        # classifier = BayesianCNN().to(device)\n",
        "        # optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
        "        # criterion = F.nll_loss\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        for epoch in trange(self.num_epochs):\n",
        "            for i, (datapoints, labels) in enumerate(self.train_loader):\n",
        "                datapoints = datapoints.unsqueeze(1).to(device)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss = self.network.sample_elbo(inputs=datapoints.to(device),\n",
        "                                  labels=labels.to(device),\n",
        "                                  criterion=criterion,\n",
        "                                  sample_nbr=3,\n",
        "                                  complexity_cost_weight=1/50000)\n",
        "                #print(loss)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                \n",
        "                iteration += 1\n",
        "                if iteration%250==0:\n",
        "                    print(loss)\n",
        "                    correct = 0\n",
        "                    total = 0\n",
        "                    print('Evaluating model on training data')\n",
        "                    eval_loader = torch.utils.data.DataLoader(\n",
        "                        dataset_train, batch_size=64, shuffle=False, drop_last=False\n",
        "                    )\n",
        "                    evaluate(self, eval_loader, data_dir, output_dir)\n",
        "                 \n",
        "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # using the confidence as estimated probaility, \n",
        "        # print(f\"x shape: {x.shape}\")\n",
        "        self.network.eval()\n",
        "        # enable_dropout(self.network)\n",
        "        # assert x.shape[1] == 28 ** 2\n",
        "        with torch.no_grad():\n",
        "          x = x.squeeze(0).unsqueeze(1)\n",
        "          estimated_probability = F.softmax(self.network(x), dim=1)\n",
        "        \n",
        "        # estimated_probability = F.softmax(self.network(x), dim=1)\n",
        "        # assert estimated_probability.shape == (x.shape[0], 10)\n",
        "        return estimated_probability\n",
        "\n",
        "def enable_dropout(model):\n",
        "    \"\"\" Function to enable the dropout layers during test-time \"\"\"\n",
        "    for m in model.modules():\n",
        "        if m.__class__.__name__.startswith('Dropout'):\n",
        "            m.train()"
      ],
      "metadata": {
        "id": "MZOc6QxOElAs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "C-wzfeYF2CpN"
      },
      "outputs": [],
      "source": [
        "class SelfMadeNetwork(nn.Module):\n",
        "    def __init__(self,\n",
        "                in_features: int, \n",
        "                out_features: int,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        # TODO General_3: Play around with the network structure.\n",
        "        # You can customize your own model here. \n",
        "        self.layer1 = None\n",
        "        self.layer2 = None\n",
        "        self.layer3 = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        class_probs = self.layer3(x)\n",
        "        return class_probs\n",
        "\n",
        "class DropoutTrainer(Framework):\n",
        "    def __init__(self, dataset_train,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__(dataset_train, *args, **kwargs)\n",
        "\n",
        "        # Hyperparameters and general parameters\n",
        "        # TODO: MC_Dropout_4. Do experiments and tune hyperparameters\n",
        "        self.batch_size = 128\n",
        "        self.learning_rate = 1e-3\n",
        "        self.num_epochs = 100\n",
        "        # torch.manual_seed(0) # set seed for reproducibility\n",
        "        \n",
        "        # TODO: MC_Dropout_1. Initialize the MC_Dropout network and optimizer here\n",
        "        # You can check the Dummy Trainer above for intuition about what to do\n",
        "        self.network = None\n",
        "        self.optimizer = None\n",
        "        \n",
        "\n",
        "    def train(self):\n",
        "        self.network.train()\n",
        "        progress_bar = trange(self.num_epochs)\n",
        "        for _ in progress_bar:\n",
        "            for batch_idx, (batch_x, batch_y) in enumerate(self.train_loader):\n",
        "                # batch_x are of shape (batch_size, 784), batch_y are of shape (batch_size,)\n",
        "\n",
        "                self.network.zero_grad()\n",
        "                # TODO: MC_Dropout_2. Implement MCDropout training here\n",
        "                # You need to calculate the loss based on the literature\n",
        "                loss = None\n",
        "\n",
        "                # Backpropagate to get the gradients\n",
        "                loss.backward()\n",
        "\n",
        "                self.optimizer.step()\n",
        "                # Update progress bar with accuracy occasionally\n",
        "                if batch_idx % self.print_interval == 0:\n",
        "                    current_logits = self.network(batch_x)\n",
        "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
        "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item())\n",
        "          \n",
        "\n",
        "    def predict_probabilities(self, x: torch.Tensor, num_sample=100) -> torch.Tensor:\n",
        "        assert x.shape[1] == 28 ** 2\n",
        "        self.network.eval()\n",
        "\n",
        "        # TODO: MC_Dropout_3. Implement your MC_dropout prediction here\n",
        "        # You need to sample from your trained model here multiple times\n",
        "        # in order to implement Monte Carlo integration\n",
        "        estimated_probability = None\n",
        "        \n",
        "        assert estimated_probability.shape == (x.shape[0], 10)  \n",
        "        return estimated_probability\n",
        "\n",
        "\n",
        "class EnsembleTrainer(Framework):\n",
        "    def __init__(self, dataset_train,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__(dataset_train, *args, **kwargs)\n",
        "\n",
        "        # Hyperparameters and general parameters\n",
        "        # TODO: Ensemble_4. Do experiments and tune hyperparameters\n",
        "        self.batch_size = 128\n",
        "        self.learning_rate = 1e-3\n",
        "        self.num_epochs = 100\n",
        "\n",
        "        # TODO: Ensemble_1.  initialize the Ensemble network list and optimizer.\n",
        "        # You can check the Dummy Trainer above for intution about what to do\n",
        "        # You need to build an ensemble of initialized networks here\n",
        "        self.EnsembleNetworks = [None]\n",
        "        self.optimizer = None\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        for network in self.EnsembleNetworks:   \n",
        "            network.train()\n",
        "            progress_bar = trange(self.num_epochs)\n",
        "            for _ in progress_bar:\n",
        "                for batch_idx, (batch_x, batch_y) in enumerate(self.train_loader):\n",
        "                    # batch_x are of shape (batch_size, 784), batch_y are of shape (batch_size,)\n",
        "\n",
        "                    network.zero_grad()\n",
        "                    # TODO: Ensemble_2. Implement Ensemble training here\n",
        "                    # You need to calculate the loss based on the literature\n",
        "                    loss = None\n",
        "\n",
        "                    # Backpropagate to get the gradients\n",
        "                    loss.backward()\n",
        "\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                    # Update progress bar with accuracy occasionally\n",
        "                    if batch_idx % self.print_interval == 0:\n",
        "                        current_logits = self.network(batch_x)\n",
        "                        current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
        "                        progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item())\n",
        "\n",
        "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.shape[1] == 28 ** 2\n",
        "        for network in self.EnsembleNetworks:  \n",
        "            network.eval() \n",
        "\n",
        "        # TODO: Ensemble_3. Implement Ensemble prediction here\n",
        "        # You need obtain predictions from each ensemble member and think about \n",
        "        # how to combine the results from each of them\n",
        "        estimated_probability = None\n",
        "        \n",
        "        assert estimated_probability.shape == (x.shape[0], 10)  \n",
        "        return estimated_probability\n",
        "\n",
        "\n",
        "class SGLDTrainer(Framework):\n",
        "    def __init__(self, dataset_train,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__(dataset_train, *args, **kwargs)\n",
        "\n",
        "        # Hyperparameters and general parameters\n",
        "        # TODO: SGLD_4. Do experiments and tune hyperparameters\n",
        "        self.batch_size = 128\n",
        "        self.learning_rate = 1e-3\n",
        "        self.num_epochs = 100\n",
        "        self.burn_in = 2\n",
        "        self.sample_interval = 3\n",
        "        self.max_size = 10\n",
        "\n",
        "\n",
        "        # TODO: SGLD_1.  initialize the SGLD network.\n",
        "        # You can check the Dummy Trainer above for intution about what to do\n",
        "        self.network = None\n",
        "        \n",
        "        # SGLD optimizer is provided\n",
        "        self.optimizer = SGLD(self.network.parameters(),lr = self.learning_rate)\n",
        "\n",
        "        # deques support bi-directional addition and deletion\n",
        "        # You can add models in the right side of a deque by append()\n",
        "        # You can delete models in the left side of a deque by popleft()\n",
        "        self.SGLDSequence = deque() \n",
        "\n",
        "    def train(self):\n",
        "        num_iter = 0\n",
        "        print('Training model')\n",
        "\n",
        "        self.network.train()\n",
        "        progress_bar = trange(self.num_epochs)\n",
        "        \n",
        "        for _ in progress_bar:\n",
        "            num_iter+=1\n",
        "\n",
        "            for batch_idx, (batch_x, batch_y) in enumerate(self.train_loader):\n",
        "                self.network.zero_grad()\n",
        "\n",
        "                # Perform forward pass\n",
        "                current_logits = self.network(batch_x)\n",
        "\n",
        "                # Calculate the loss\n",
        "                # TODO: SGLD_1. Implement SGLD training here\n",
        "                # You need to calculate the loss based on the literature\n",
        "                loss = None\n",
        "\n",
        "                # Backpropagate to get the gradients\n",
        "                loss.backward()\n",
        "\n",
        "                self.optimizer.step()\n",
        "\n",
        "                if batch_idx % self.print_interval == 0:\n",
        "                    current_logits = self.network(batch_x)\n",
        "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
        "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item())\n",
        "  \n",
        "            # TODO: SGLD_2. save the model samples if it satisfies the following conditions:\n",
        "            # We are 1) past the burn-in epochs and 2) reached one of the regular sampling intervals we save the model at\n",
        "            # If the self.SGLDSequence already exceeded the maximum length then we have to delete the oldest model\n",
        "            if None:\n",
        "                self.SGLDSequence # add model\n",
        "            if None:\n",
        "                self.SGLDSequence # remove model\n",
        "\n",
        "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        assert x.shape[1] == 28 ** 2\n",
        "        self.network.eval()\n",
        "\n",
        "        # TODO SGLD_3: Implement SGLD predictions here\n",
        "        # You need to obtain the prediction from each network\n",
        "        # in SGLDSequence and combine the predictions\n",
        "        estimated_probability = None\n",
        "        \n",
        "        assert estimated_probability.shape == (x.shape[0], 10)  \n",
        "        return estimated_probability\n",
        "\n",
        "\n",
        "class BackpropTrainer(Framework):\n",
        "    def __init__(self, dataset_train,  *args, **kwargs):\n",
        "        super().__init__(dataset_train, *args, **kwargs)\n",
        "\n",
        "        # Hyperparameters and general parameters\n",
        "        # TODO: Backprop_7 Tune parameters and add more if necessary\n",
        "        self.hidden_features=(100,100)\n",
        "        self.batch_size = 128\n",
        "        self.num_epochs = 10\n",
        "        learning_rate = 1e-3\n",
        "\n",
        "        self.network = BayesNet(in_features=28 * 28, hidden_features=self.hidden_features, out_features=10)\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
        "        self.train_loader = torch.utils.data.DataLoader(\n",
        "            dataset_train, batch_size=self.batch_size, shuffle=True, drop_last=True\n",
        "            )\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.network.train()\n",
        "\n",
        "        progress_bar = trange(self.num_epochs)\n",
        "        for _ in progress_bar:\n",
        "            num_batches = len(self.train_loader)\n",
        "            for batch_idx, (batch_x, batch_y) in enumerate(self.train_loader):\n",
        "                # batch_x are of shape (batch_size, 784), batch_y are of shape (batch_size,)\n",
        "\n",
        "                self.network.zero_grad()\n",
        "\n",
        "                # TODO: Backprop_6: Implement Bayes by backprop training here\n",
        "                loss = None\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Update progress bar with accuracy occasionally\n",
        "                if batch_idx % self.print_interval == 0:\n",
        "                    current_logits, _, _ = self.network(batch_x)\n",
        "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
        "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item())\n",
        "\n",
        "\n",
        "    def predict_probabilities(self, x: torch.Tensor, num_mc_samples: int = 10) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Predict class probabilities for the given features by sampling from this BNN.\n",
        "\n",
        "        :param x: Features to predict on, float tensor of shape (batch_size, in_features)\n",
        "        :param num_mc_samples: Number of MC samples to take for prediction\n",
        "        :return: Predicted class probabilities, float tensor of shape (batch_size, 10)\n",
        "            such that the last dimension sums up to 1 for each row\n",
        "        \"\"\"\n",
        "        probability_samples = torch.stack([F.softmax(self.network(x)[0], dim=1) for _ in range(num_mc_samples)], dim=0)\n",
        "        estimated_probability = torch.mean(probability_samples, dim=0)\n",
        "\n",
        "        assert estimated_probability.shape == (x.shape[0], 10)\n",
        "        assert torch.allclose(torch.sum(estimated_probability, dim=1), torch.tensor(1.0))\n",
        "        return estimated_probability\n",
        "\n",
        "class BayesianLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Module implementing a single Bayesian feedforward layer.\n",
        "    It maintains a prior and variational posterior for the weights (and biases)\n",
        "    and uses sampling to approximate the gradients via Bayes by backprop.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True):\n",
        "        \"\"\"\n",
        "        Create a BayesianLayer.\n",
        "\n",
        "        :param in_features: Number of input features\n",
        "        :param out_features: Number of output features\n",
        "        :param bias: If true, use a bias term (i.e., affine instead of linear transformation)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.use_bias = bias\n",
        "\n",
        "        # Background Pytorch will backpropogate gradients to an object initialized with\n",
        "        # torch.Parameter(...) and the object will be updated when computing loss.backwards()\n",
        "        # during training. This will not happen for a torch.Tensor(...) object, which is by default a constant. \n",
        "\n",
        "        # TODO: Backprop_1. Create a suitable prior for weights and biases as an instance of ParameterDistribution.\n",
        "        #  You can use the same prior for both weights and biases, but are also free to experiment with other priors.\n",
        "        #  You can create constants using torch.tensor(...).\n",
        "        #  Do NOT use torch.Parameter(...) here since the prior should not be optimized!\n",
        "        #  Example: self.prior = MyPrior(torch.tensor(0.0), torch.tensor(1.0))\n",
        "        self.prior = None\n",
        "        assert isinstance(self.prior, ParameterDistribution)\n",
        "        assert not any(True for _ in self.prior.parameters()), 'Prior cannot have parameters'\n",
        "\n",
        "        # TODO: Backprop_1. Create a suitable variational posterior for weights as an instance of ParameterDistribution.\n",
        "        #  You need to create separate ParameterDistribution instances for weights and biases,\n",
        "        #  but you can use the same family of distributions for each if you want.\n",
        "        #  IMPORTANT: You need to create a nn.Parameter(...) for each parameter\n",
        "        #  and add those parameters as an attribute in the ParameterDistribution instances.\n",
        "        #  If you forget to do so, PyTorch will not be able to optimize your variational posterior.\n",
        "        # The variational posterior for weights is created here. For the biases it is created further down. \n",
        "        #  Example: self.weights_var_posterior = MyPosterior(\n",
        "        #      torch.nn.Parameter(torch.zeros((out_features, in_features))),\n",
        "        #      torch.nn.Parameter(torch.ones((out_features, in_features)))\n",
        "        #  )\n",
        "        self.weights_var_posterior = None\n",
        "\n",
        "        assert isinstance(self.weights_var_posterior, ParameterDistribution)\n",
        "        assert any(True for _ in self.weights_var_posterior.parameters()), 'Weight posterior must have parameters'\n",
        "\n",
        "        if self.use_bias:\n",
        "            # TODO: Backprop_1. Similarly as you did above for the weights, create the bias variational posterior instance here.\n",
        "            #  Make sure to follow the same rules as for the weight variational posterior.\n",
        "            self.bias_var_posterior = None\n",
        "            assert isinstance(self.bias_var_posterior, ParameterDistribution)\n",
        "            assert any(True for _ in self.bias_var_posterior.parameters()), 'Bias posterior must have parameters'\n",
        "        else:\n",
        "            self.bias_var_posterior = None\n",
        "\n",
        "\n",
        "    def forward(self, inputs: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Perform one forward pass through this layer.\n",
        "        If you need to sample weights from the variational posterior, you can do it here during the forward pass.\n",
        "        Just make sure that you use the same weights to approximate all quantities\n",
        "        present in a single Bayes by backprop sampling step.\n",
        "\n",
        "        :param inputs: Flattened input images as a (batch_size, in_features) float tensor\n",
        "        :return: 3-tuple containing\n",
        "            i) transformed features using stochastic weights from the variational posterior,\n",
        "            ii) sample of the log-prior probability, and\n",
        "            iii) sample of the log-variational-posterior probability\n",
        "        \"\"\"\n",
        "        # TODO: Backprop_2. Perform a forward pass as described in this method's docstring.\n",
        "        #  Make sure to check whether `self.use_bias` is True,\n",
        "        #  and if yes, include the bias as well.\n",
        "        log_prior = torch.tensor(0.0)\n",
        "        log_variational_posterior = torch.tensor(0.0)\n",
        "        weights = None\n",
        "        bias = None\n",
        "\n",
        "        return F.linear(inputs, weights, bias), log_prior, log_variational_posterior\n",
        "\n",
        "\n",
        "class BayesNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Module implementing a Bayesian feedforward neural network using BayesianLayer objects.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, hidden_features: typing.Tuple[int, ...], out_features: int):\n",
        "        \"\"\"\n",
        "        Create a BNN.\n",
        "\n",
        "        :param in_features: Number of input features\n",
        "        :param hidden_features: Tuple where each entry corresponds to a (Bayesian) hidden layer with\n",
        "            the corresponding number of features.\n",
        "        :param out_features: Number of output features\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        feature_sizes = (in_features,) + hidden_features + (out_features,)\n",
        "        num_affine_maps = len(feature_sizes) - 1\n",
        "        self.layers = nn.ModuleList([\n",
        "            BayesianLayer(feature_sizes[idx], feature_sizes[idx + 1], bias=True)\n",
        "            for idx in range(num_affine_maps)\n",
        "        ])\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> typing.Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Perform one forward pass through the BNN using a single set of weights\n",
        "        sampled from the variational posterior.\n",
        "\n",
        "        :param x: Input features, float tensor of shape (batch_size, in_features)\n",
        "        :return: 3-tuple containing\n",
        "            i) output features using stochastic weights from the variational posterior,\n",
        "            ii) sample of the log-prior probability, and\n",
        "            iii) sample of the log-variational-posterior probability\n",
        "        \"\"\"\n",
        "\n",
        "        # TODO: Backprop_3. Perform a full pass through your BayesNet as described in this method's docstring.\n",
        "        #  You can look at Dummy Trainer to get an idea how a forward pass might look like.\n",
        "        #  Don't forget to apply your activation function in between BayesianLayers!\n",
        "        log_prior = torch.tensor(0.0)\n",
        "        log_variational_posterior = torch.tensor(0.0)\n",
        "        output_features = None\n",
        "\n",
        "        return output_features, log_prior, log_variational_posterior\n",
        "\n",
        "\n",
        "    def predict_probabilities(self, x: torch.Tensor, num_mc_samples: int = 50) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Predict class probabilities for the given features by sampling from this BNN.\n",
        "\n",
        "        :param x: Features to predict on, float tensor of shape (batch_size, in_features)\n",
        "        :param num_mc_samples: Number of MC samples to take for prediction\n",
        "        :return: Predicted class probabilities, float tensor of shape (batch_size, 10)\n",
        "            such that the last dimension sums up to 1 for each row\n",
        "        \"\"\"\n",
        "        probability_samples = torch.stack([F.softmax(self.forward(x)[0], dim=1) for _ in range(num_mc_samples)], dim=0)\n",
        "        estimated_probability = torch.mean(probability_samples, dim=0)\n",
        "\n",
        "        assert estimated_probability.shape == (x.shape[0], 10)\n",
        "        assert torch.allclose(torch.sum(estimated_probability, dim=1), torch.tensor(1.0))\n",
        "        return estimated_probability\n",
        "\n",
        "\n",
        "class UnivariateGaussian(ParameterDistribution):\n",
        "    \"\"\"\n",
        "    Univariate Gaussian distribution.\n",
        "    For multivariate data, this assumes all elements to be i.i.d.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mu: torch.Tensor, sigma: torch.Tensor):\n",
        "        super(UnivariateGaussian, self).__init__()  # always make sure to include the super-class init call!\n",
        "        assert mu.size() == () and sigma.size() == ()\n",
        "        assert sigma > 0\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "\n",
        "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
        "        # TODO: Backprop_4. You need to complete the log likelihood function \n",
        "        # for the Univariate Gaussian distribution. \n",
        "        return 0.0\n",
        "\n",
        "    def sample(self) -> torch.Tensor:\n",
        "        # TODO: Backprop_4. You need to complete the sample function \n",
        "        # for the Univariate Gaussian distribution. \n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class MultivariateDiagonalGaussian(ParameterDistribution):\n",
        "    \"\"\"\n",
        "    Multivariate diagonal Gaussian distribution,\n",
        "    i.e., assumes all elements to be independent Gaussians\n",
        "    but with different means and standard deviations.\n",
        "    This parameterizes the standard deviation via a parameter rho as\n",
        "    sigma = softplus(rho).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor):\n",
        "        super(MultivariateDiagonalGaussian, self).__init__()  # always make sure to include the super-class init call!\n",
        "        assert mu.size() == rho.size()\n",
        "        self.mu = mu\n",
        "        self.rho = rho\n",
        "\n",
        "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
        "        # TODO: Backprop_5. You need to complete the log likelihood function \n",
        "        # for the Multivariate DiagonalGaussian Gaussian distribution. \n",
        "        return 0.0\n",
        "\n",
        "    def sample(self) -> torch.Tensor:\n",
        "        # TODO: Backprop_5. You need to complete the sample function \n",
        "        # for the Multivariate DiagonalGaussian Gaussian distribution. \n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "\n",
        "class SelfTrainer(Framework):\n",
        "    def __init__(self, dataset_train, print_interval=50,*args, **kwargs):\n",
        "        \"\"\"\n",
        "        Basic Framework for creating your own bayesian neural network trainer.\n",
        "        \"\"\"\n",
        "        self.train_set = dataset_train\n",
        "        self.print_interval = print_interval  # number of batches until updated metrics are displayed during training\n",
        "\n",
        "    def train(self, num_epochs):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def predict(self, data_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict the class probabilities using your trained model.\n",
        "        This method should return an (num_samples, 10) NumPy float array\n",
        "        such that the second dimension sums up to 1 for each row.\n",
        "\n",
        "        :param data_loader: Data loader yielding the samples to predict on\n",
        "        :return: (num_samples, 10) NumPy float array where the second dimension sums up to 1 for each row\n",
        "        \"\"\"\n",
        "        probability_batches = []\n",
        "        \n",
        "        for batch_x, batch_y in tqdm.tqdm(data_loader):\n",
        "            current_probabilities = self.predict_probabilities(batch_x).detach().cpu().numpy()\n",
        "            probability_batches.append(current_probabilities)\n",
        "\n",
        "        output = np.concatenate(probability_batches, axis=0)\n",
        "        assert isinstance(output, np.ndarray)\n",
        "        assert output.ndim == 2 and output.shape[1] == 10\n",
        "        assert np.allclose(np.sum(output, axis=1), 1.0)\n",
        "        return output\n",
        "\n",
        "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "def evaluate(model:Framework, eval_loader: torch.utils.data.DataLoader, data_dir: str, output_dir: str):\n",
        "    \"\"\"\n",
        "    Evaluate your model.\n",
        "    :param model: Trained model to evaluate\n",
        "    :param eval_loader: Data loader containing the training set for evaluation\n",
        "    :param data_dir: Data directory from which additional datasets are loaded\n",
        "    :param output_dir: Directory into which plots are saved\n",
        "    \"\"\"\n",
        "    print(\"evaulating\")\n",
        "    # Predict class probabilities on test data\n",
        "    predicted_probabilities = model.predict(eval_loader)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
        "    actual_classes = eval_loader.dataset.tensors[1].detach().cpu().numpy()\n",
        "    accuracy = np.mean((predicted_classes == actual_classes)) \n",
        "    ece_score = ece(predicted_probabilities, actual_classes)\n",
        "    print(f'Accuracy: {accuracy.item():.3f}, ECE score: {ece_score:.3f}')\n",
        "\n",
        "    # TODO: Reliability_diagram_3. draw reliability diagram on Test Data\n",
        "    # You can uncomment the below code to make it run. You can learn from\n",
        "    # the graph about how to improve your model. Remember first to complete \n",
        "    # the function of calc_calibration_curve.\n",
        "  \n",
        "    # print('Plotting reliability diagram on Test Dataset')\n",
        "    # out = calc_calibration_curve(predicted_probabilities, actual_classes, num_bins = 30)\n",
        "    # fig = draw_reliability_diagram(out)\n",
        "    # fig.savefig(os.path.join(output_dir, 'reliability-diagram.pdf'))\n",
        "\n",
        "    if EXTENDED_EVALUATION:\n",
        "        if not os.path.isdir(output_dir):\n",
        "            os.mkdir(output_dir)\n",
        "        ### draw reliability diagram\n",
        "        print('Plotting reliability diagram')\n",
        "        reliability_diagram_datax = np.load(os.path.join(data_dir, 'reliability_diagram_data_x.npy'))\n",
        "        reliability_diagram_datay = np.load(os.path.join(data_dir, 'reliability_diagram_data_y.npy'))\n",
        "        for i in range(3):\n",
        "            demo_prob_i = reliability_diagram_datax[i]\n",
        "            demo_label_i = reliability_diagram_datay[i]\n",
        "            out = calc_calibration_curve(demo_prob_i, demo_label_i, num_bins = 9)\n",
        "            fig = draw_reliability_diagram(out)\n",
        "            fig.savefig(os.path.join(output_dir, str(i)+'_reliability-diagram.pdf'))\n",
        "        \n",
        "        eval_samples = eval_loader.dataset.tensors[0].detach().cpu().numpy()\n",
        "\n",
        "        # Determine confidence per sample and sort accordingly\n",
        "        confidences = np.max(predicted_probabilities, axis=1)\n",
        "        sorted_confidence_indices = np.argsort(confidences)\n",
        "\n",
        "        # Plot samples your model is most confident about\n",
        "        print('Plotting most confident MNIST predictions')\n",
        "        most_confident_indices = sorted_confidence_indices[-10:]\n",
        "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
        "        for row in range(0, 4, 2):\n",
        "            for col in range(5):\n",
        "                sample_idx = most_confident_indices[5 * row // 2 + col]\n",
        "                ax[row, col].imshow(np.reshape(eval_samples[sample_idx], (28, 28)), cmap='gray')\n",
        "                ax[row, col].set_axis_off()\n",
        "                ax[row + 1, col].set_title(f'predicted {predicted_classes[sample_idx]}, actual {actual_classes[sample_idx]}')\n",
        "                bar_colors = ['C0'] * 10\n",
        "                bar_colors[actual_classes[sample_idx]] = 'C1'\n",
        "                ax[row + 1, col].bar(\n",
        "                    np.arange(10), predicted_probabilities[sample_idx], tick_label=np.arange(10), color=bar_colors\n",
        "                )\n",
        "        fig.suptitle('Most confident predictions', size=20)\n",
        "        fig.savefig(os.path.join(output_dir, 'mnist_most_confident.pdf'))\n",
        "\n",
        "        # Plot samples your model is least confident about\n",
        "        print('Plotting least confident MNIST predictions')\n",
        "        least_confident_indices = sorted_confidence_indices[:10]\n",
        "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
        "        for row in range(0, 4, 2):\n",
        "            for col in range(5):\n",
        "                sample_idx = least_confident_indices[5 * row // 2 + col]\n",
        "                ax[row, col].imshow(np.reshape(eval_samples[sample_idx], (28, 28)), cmap='gray')\n",
        "                ax[row, col].set_axis_off()\n",
        "                ax[row + 1, col].set_title(f'predicted {predicted_classes[sample_idx]}, actual {actual_classes[sample_idx]}')\n",
        "                bar_colors = ['C0'] * 10\n",
        "                bar_colors[actual_classes[sample_idx]] = 'C1'\n",
        "                ax[row + 1, col].bar(\n",
        "                    np.arange(10), predicted_probabilities[sample_idx], tick_label=np.arange(10), color=bar_colors\n",
        "                )\n",
        "        fig.suptitle('Least confident predictions', size=20)\n",
        "        fig.savefig(os.path.join(output_dir, 'mnist_least_confident.pdf'))\n",
        "\n",
        "        print('Plotting ambiguous and rotated MNIST confidences')\n",
        "        ambiguous_samples = torch.from_numpy(np.load(os.path.join(data_dir, 'test_x.npz'))['test_x']).reshape([-1, 784])[:10]\n",
        "        ambiguous_dataset = torch.utils.data.TensorDataset(ambiguous_samples, torch.zeros(10))\n",
        "        ambiguous_loader = torch.utils.data.DataLoader(\n",
        "            ambiguous_dataset, batch_size=10, shuffle=False, drop_last=False\n",
        "        )\n",
        "        ambiguous_predicted_probabilities = model.predict(ambiguous_loader)\n",
        "        ambiguous_predicted_classes = np.argmax(ambiguous_predicted_probabilities, axis=1)\n",
        "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
        "        for row in range(0, 4, 2):\n",
        "            for col in range(5):\n",
        "                sample_idx = 5 * row // 2 + col\n",
        "                ax[row, col].imshow(np.reshape(ambiguous_samples[sample_idx], (28, 28)), cmap='gray')\n",
        "                ax[row, col].set_axis_off()\n",
        "                ax[row + 1, col].set_title(f'predicted {ambiguous_predicted_classes[sample_idx]}')\n",
        "                ax[row + 1, col].bar(\n",
        "                    np.arange(10), ambiguous_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
        "                )\n",
        "        fig.suptitle('Predictions on ambiguous and rotated MNIST', size=20)\n",
        "        fig.savefig(os.path.join(output_dir, 'ambiguous_rotated_mnist.pdf'))\n",
        "\n",
        "\n",
        "        # Do the same evaluation as on MNIST also on FashionMNIST\n",
        "        print('Predicting on FashionMNIST data')\n",
        "        fmnist_samples = torch.from_numpy(np.load(os.path.join(data_dir, 'fmnist.npz'))['x_test'])#.reshape([-1, 784])\n",
        "        fmnist_dataset = torch.utils.data.TensorDataset(fmnist_samples, torch.zeros(fmnist_samples.shape[0]))\n",
        "        fmnist_loader = torch.utils.data.DataLoader(\n",
        "            fmnist_dataset, batch_size=64, shuffle=False, drop_last=False\n",
        "        )\n",
        "        fmnist_predicted_probabilities = model.predict(fmnist_loader)\n",
        "        fmnist_predicted_classes = np.argmax(fmnist_predicted_probabilities, axis=1)\n",
        "        fmnist_confidences = np.max(fmnist_predicted_probabilities, axis=1)\n",
        "        fmnist_sorted_confidence_indices = np.argsort(fmnist_confidences)\n",
        "\n",
        "        # Plot FashionMNIST samples your model is most confident about\n",
        "        print('Plotting most confident FashionMNIST predictions')\n",
        "        most_confident_indices = fmnist_sorted_confidence_indices[-10:]\n",
        "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
        "        for row in range(0, 4, 2):\n",
        "            for col in range(5):\n",
        "                sample_idx = most_confident_indices[5 * row // 2 + col]\n",
        "                ax[row, col].imshow(np.reshape(fmnist_samples[sample_idx], (28, 28)), cmap='gray')\n",
        "                ax[row, col].set_axis_off()\n",
        "                ax[row + 1, col].set_title(f'predicted {fmnist_predicted_classes[sample_idx]}')\n",
        "                ax[row + 1, col].bar(\n",
        "                    np.arange(10), fmnist_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
        "                )\n",
        "        fig.suptitle('Most confident predictions', size=20)\n",
        "        fig.savefig(os.path.join(output_dir, 'fashionmnist_most_confident.pdf'))\n",
        "\n",
        "        # Plot FashionMNIST samples your model is least confident about\n",
        "        print('Plotting least confident FashionMNIST predictions')\n",
        "        least_confident_indices = fmnist_sorted_confidence_indices[:10]\n",
        "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
        "        for row in range(0, 4, 2):\n",
        "            for col in range(5):\n",
        "                sample_idx = least_confident_indices[5 * row // 2 + col]\n",
        "                ax[row, col].imshow(np.reshape(fmnist_samples[sample_idx], (28, 28)), cmap='gray')\n",
        "                ax[row, col].set_axis_off()\n",
        "                ax[row + 1, col].set_title(f'predicted {fmnist_predicted_classes[sample_idx]}')\n",
        "                ax[row + 1, col].bar(\n",
        "                    np.arange(10), fmnist_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
        "                )\n",
        "        fig.suptitle('Least confident predictions', size=20)\n",
        "        fig.savefig(os.path.join(output_dir, 'fashionmnist_least_confident.pdf'))\n",
        "\n",
        "        print('Determining suitability of your model for OOD detection')\n",
        "        all_confidences = np.concatenate([confidences, fmnist_confidences])\n",
        "        dataset_labels = np.concatenate([np.ones_like(confidences), np.zeros_like(fmnist_confidences)])\n",
        "        print(\n",
        "            'AUROC for MNIST vs. FashionMNIST OOD detection based on confidence: '\n",
        "            f'{roc_auc_score(dataset_labels, all_confidences):.3f}'\n",
        "        )\n",
        "        print(\n",
        "            'AUPRC for MNIST vs. FashionMNIST OOD detection based on confidence: '\n",
        "            f'{average_precision_score(dataset_labels, all_confidences):.3f}'\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_loader2 = torch.utils.data.DataLoader(dataset_train, batch_size=64, shuffle=True, drop_last=True)\n",
        "# for i, (datapoints, labels) in enumerate(train_loader2):\n",
        "#   print(f\"i: {i}, datapoints: {datapoints.unsqueeze(1).shape}, {labels.shape}\")\n",
        "#   break"
      ],
      "metadata": {
        "id": "sqALomTFywNH"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "data_dir = os.curdir\n",
        "output_dir = os.curdir\n",
        "raw_train_data = np.load(os.path.join(data_dir, 'train_data.npz'))\n",
        "x_train = torch.from_numpy(raw_train_data['train_x']).to(device)\n",
        "y_train = torch.from_numpy(raw_train_data['train_y']).long().to(device)\n",
        "dataset_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "\n",
        "# Run actual solution\n",
        "trainer = run_solution(dataset_train, data_dir=data_dir, output_dir=output_dir)\n"
      ],
      "metadata": {
        "id": "O5m1D5W5i-Lv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e0bd3d7-1215-4e17-f80b-09cd9801c129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model Dummy_Trainer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/100 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class SGLD(SGD):\n",
        "    \"\"\"Implementation of SGLD algorithm.\n",
        "    References\n",
        "    ----------\n",
        "        \n",
        "    \"\"\"\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        \"\"\"See `torch.optim.step.\"\"\"\n",
        "        loss = super().step(closure)\n",
        "        for group in self.param_groups:\n",
        "            weight_decay = group['weight_decay']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None:\n",
        "                    continue\n",
        "                grad_p = p.grad.data\n",
        "                if weight_decay!=0:\n",
        "                    grad_p.add_(alpha=weight_decay,other=p.data)\n",
        "                langevin_noise = 0.1*torch.randn_like(p.data).mul_(group['lr']**0.5) #  use weight 0.1 to balance the noise\n",
        "                p.data.add_(grad_p,alpha=-0.5*group['lr'])\n",
        "                if torch.isnan(p.data).any(): \n",
        "                    exit('Exist NaN param after SGLD, Try to tune the parameter')\n",
        "                if torch.isinf(p.data).any(): \n",
        "                    exit('Exist Inf param after SGLD, Try to tune the parameter')\n",
        "                p.data.add_(langevin_noise)\n",
        "        return loss\n",
        "\n",
        "def ece(predicted_probabilities: np.ndarray, labels: np.ndarray, n_bins: int = 30) -> float:\n",
        "    \"\"\"\n",
        "    Computes the Expected Calibration Error (ECE).\n",
        "    Many options are possible; in this implementation, we provide a simple version.\n",
        "\n",
        "    Using a uniform binning scheme on the full range of probabilities, zero\n",
        "    to one, we bin the probabilities of the predicted label only (ignoring\n",
        "    all other probabilities). For the ith bin, we compute the avg predicted\n",
        "    probability, p_i, and the bin's total accuracy, a_i.\n",
        "    We then compute the ith calibration error of the bin, |p_i - a_i|.\n",
        "    The final returned value is the weighted average of calibration errors of each bin.\n",
        "\n",
        "    :param predicted_probabilities: Predicted probabilities, float array of shape (num_samples, num_classes)\n",
        "    :param labels: True labels, int tensor of shape (num_samples,) with each entry in {0, ..., num_classes - 1}\n",
        "    :param n_bins: Number of bins for histogram binning\n",
        "    :return: ECE score as a float\n",
        "    \"\"\"\n",
        "    num_samples, num_classes = predicted_probabilities.shape\n",
        "\n",
        "    # Predictions are the classes with highest probability\n",
        "    predictions = np.argmax(predicted_probabilities, axis=1)\n",
        "    prediction_confidences = predicted_probabilities[range(num_samples), predictions]\n",
        "\n",
        "    # Use uniform bins on the range of probabilities, i.e. closed interval [0.,1.]\n",
        "    bin_upper_edges = np.histogram_bin_edges([], bins=n_bins, range=(0., 1.))\n",
        "    bin_upper_edges = bin_upper_edges[1:]  # bin_upper_edges[0] = 0.\n",
        "\n",
        "    probs_as_bin_num = np.digitize(prediction_confidences, bin_upper_edges)\n",
        "    sums_per_bin = np.bincount(probs_as_bin_num, minlength=n_bins, weights=prediction_confidences)\n",
        "    sums_per_bin = sums_per_bin.astype(np.float32)\n",
        "\n",
        "    total_per_bin = np.bincount(probs_as_bin_num, minlength=n_bins) \\\n",
        "        + np.finfo(sums_per_bin.dtype).eps  # division by zero\n",
        "    avg_prob_per_bin = sums_per_bin / total_per_bin\n",
        "\n",
        "    onehot_labels = np.eye(num_classes)[labels]\n",
        "    accuracies = onehot_labels[range(num_samples), predictions]  # accuracies[i] is 0 or 1\n",
        "    accuracies_per_bin = np.bincount(probs_as_bin_num, weights=accuracies, minlength=n_bins) / total_per_bin\n",
        "\n",
        "    prob_of_being_in_a_bin = total_per_bin / float(num_samples)\n",
        "\n",
        "    ece_ret = np.abs(accuracies_per_bin - avg_prob_per_bin) * prob_of_being_in_a_bin\n",
        "    ece_ret = np.sum(ece_ret)\n",
        "    return float(ece_ret)\n",
        "\n",
        "\n",
        "class ParameterDistribution(torch.nn.Module, metaclass=abc.ABCMeta):\n",
        "    \"\"\"\n",
        "    Abstract class that models a distribution over model parameters,\n",
        "    usable for Bayes by backprop.\n",
        "    You can implement this class using any distribution you want\n",
        "    and try out different priors and variational posteriors.\n",
        "    All torch.nn.Parameter that you add in the __init__ method of this class\n",
        "    will automatically be registered and know to PyTorch.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def log_likelihood(self, values: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Calculate the log-likelihood of the given values\n",
        "        :param values: Values to calculate the log-likelihood on\n",
        "        :return: Log-likelihood\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def sample(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Sample from this distribution.\n",
        "        Note that you only need to implement this method for variational posteriors, not priors.\n",
        "\n",
        "        :return: Sample from this distribution. The sample shape depends on your semantics.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def forward(self, values: torch.Tensor) -> torch.Tensor:\n",
        "        # DO NOT USE THIS METHOD\n",
        "        # We only implement it since torch.nn.Module requires a forward method\n",
        "        warnings.warn('ParameterDistribution should not be called! Use its explicit methods!')\n",
        "        return self.log_likelihood(values)\n",
        "\n",
        "\n",
        "def draw_reliability_diagram(out,\n",
        "                            title=\"Reliability Diagram\", \n",
        "                            xlabel=\"Confidence\", \n",
        "                            ylabel=\"Accuracy\"):\n",
        "    \"\"\"Draws a reliability diagram into a subplot.\"\"\"\n",
        "    fig,ax = plt.subplots()\n",
        "    plt.tight_layout()\n",
        "    accuracies =  out['calib_accuracy']\n",
        "    confidences = out['calib_confidence']\n",
        "    counts = out['p']\n",
        "    bins = out['bins']\n",
        "\n",
        "    bin_size = 1.0 / len(counts)\n",
        "    positions = bins[:-1] + bin_size/2.0\n",
        "\n",
        "    widths = bin_size\n",
        "    alphas = 0.3\n",
        "\n",
        "    colors = np.zeros((len(counts), 4))\n",
        "    colors[:, 0] = 240 / 255.\n",
        "    colors[:, 1] = 60 / 255.\n",
        "    colors[:, 2] = 60 / 255.\n",
        "    colors[:, 3] = alphas\n",
        "\n",
        "    gap_plt = ax.bar(positions, np.abs(accuracies - confidences), \n",
        "                     bottom=np.minimum(accuracies, confidences), width=widths,\n",
        "                     edgecolor=colors, color=colors, linewidth=1, label=\"Gap\")\n",
        "\n",
        "    acc_plt = ax.bar(positions, 0, bottom=accuracies, width=widths,\n",
        "                     edgecolor=\"black\", color=\"black\", alpha=1.0, linewidth=3,\n",
        "                     label=\"Accuracy\")\n",
        "\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.plot([0,1], [0,1], linestyle = \"--\", color=\"gray\")\n",
        "    \n",
        "\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\\\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.legend(handles=[gap_plt, acc_plt])\n",
        "    return fig\n",
        "\n",
        "\n",
        "def draw_confidence_histogram(out, \n",
        "                                draw_averages=True,\n",
        "                                title=\"Confidence Diagram\",\n",
        "                                xlabel=\"Confidence\",\n",
        "                                ylabel=\"Count\"):\n",
        "    \"\"\"Draws a confidence histogram into a subplot.\"\"\"\n",
        "\n",
        "    fig,ax = plt.subplots()\n",
        "    zs = out['p']\n",
        "    bins = out['bins']\n",
        "    bin_lowers = bins[:-1]\n",
        "    bin_uppers = bins[1:]\n",
        "    bin_middles = (bin_lowers + bin_uppers)/2\n",
        "\n",
        "    bin_size = 1.0 / len(zs)\n",
        "\n",
        "    ax.bar(bin_middles, zs, width=bin_size * 0.9)\n",
        "   \n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    return fig\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UcE9gM8FjTLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHxmf073FJA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}