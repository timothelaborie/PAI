{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JwCZaqIeaCY7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import typing\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import trange\n",
        "import tqdm\n",
        "from torch.distributions import Poisson\n",
        "from collections import deque\n",
        "import copy\n",
        "\n",
        "from util import ece, ParameterDistribution, draw_reliability_diagram, draw_confidence_histogram, SGLD\n",
        "from enum import Enum\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DECE(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes DECE loss (differentiable expected calibration error).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, num_bins, t_a, t_b):\n",
        "        super(DECE, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_bins = num_bins\n",
        "        self.t_a = t_a\n",
        "        self.t_b = t_b\n",
        "\n",
        "    def one_hot(self, indices, depth):\n",
        "        \"\"\"\n",
        "        Returns a one-hot tensor.\n",
        "        This is a PyTorch equivalent of Tensorflow's tf.one_hot.\n",
        "        Parameters:\n",
        "        indices:  a (n_batch, m) Tensor or (m) Tensor.\n",
        "        depth: a scalar. Represents the depth of the one hot dimension.\n",
        "        Returns: a (n_batch, m, depth) Tensor or (m, depth) Tensor.\n",
        "        \"\"\"\n",
        "        encoded_indicies = torch.zeros(\n",
        "            indices.size() + torch.Size([depth])).to(device=self.device)\n",
        "        index = indices.view(indices.size() + torch.Size([1]))\n",
        "        encoded_indicies = encoded_indicies.scatter_(1, index, 1)\n",
        "\n",
        "        return encoded_indicies\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if input.dim() > 2:\n",
        "            # N,C,H,W => N,C,H*W\n",
        "            input = input.view(input.size(0), input.size(1), -1)\n",
        "            input = input.transpose(1, 2)    # N,C,H*W => N,H*W,C\n",
        "            input = input.contiguous().view(-1, input.size(2))   # N,H*W,C => N*H*W,C\n",
        "\n",
        "        # For CIFAR-10 and CIFAR-100, target.shape is [N] to begin with\n",
        "        target = target.view(-1)\n",
        "\n",
        "        predicted_probs = F.softmax(input, dim=1)\n",
        "\n",
        "        cut_points = torch.linspace(0, 1, self.num_bins + 1)[:-1].to(device=self.device)\n",
        "        W = torch.reshape(torch.linspace(1.0, self.num_bins, self.num_bins).to(device=self.device), [1, -1])\n",
        "        b = torch.cumsum(-cut_points, 0)\n",
        "\n",
        "        confidences = torch.max(predicted_probs, dim=1, keepdim=True)[0]\n",
        "        h = torch.matmul(confidences, W) + b\n",
        "        h = h / self.t_b\n",
        "\n",
        "        bin_probs = F.softmax(h, dim=1)\n",
        "\n",
        "        # smoothen the probabilities to avoid zeros\n",
        "        eps = 1e-6\n",
        "        bin_probs = bin_probs + eps\n",
        "        # normalize the probabilities to sum to one across bins\n",
        "        bin_probs = bin_probs / (1.0 + (self.num_bins + 1) * eps)\n",
        "\n",
        "        # calculate bin confidences\n",
        "        bin_confs = torch.div(bin_probs.transpose(0, 1).matmul(confidences).view(-1),\n",
        "                              torch.sum(bin_probs, dim=0))\n",
        "        # all-pairs approach\n",
        "        batch_pred_diffs = torch.unsqueeze(predicted_probs, dim=2) - torch.unsqueeze(predicted_probs, dim=1)\n",
        "        # computing pairwise differences, i.e., Sij or Sxy\n",
        "        if str(self.device) == 'cpu':\n",
        "            gpu = False\n",
        "        else:\n",
        "            gpu = True\n",
        "        # using {-1.0*} may lead to a poor performance when compared with the above way;\n",
        "        batch_indicators = robust_sigmoid(torch.transpose(batch_pred_diffs, dim0=1, dim1=2), self.t_a, gpu)\n",
        "\n",
        "        # get approximated rank positions, i.e., hat_pi(x)\n",
        "        ranks_all = torch.sum(batch_indicators, dim=2) + 0.5\n",
        "        # the ranks go from 1 to C, with 1 being the best rank\n",
        "        true_ranks = ranks_all[torch.arange(ranks_all.size(0)), target]\n",
        "        accs = F.relu(2.0 - true_ranks)\n",
        "        bin_accs = torch.div(bin_probs.transpose(0, 1).matmul(accs).view(-1),\n",
        "                                torch.sum(bin_probs, dim=0))\n",
        "\n",
        "        # calculate overall ECE for the whole batch\n",
        "        ece = torch.sum(torch.sum(bin_probs, dim=0) * torch.abs(bin_accs - bin_confs) / bin_probs.shape[0], dim=0)\n",
        "        return ece"
      ],
      "metadata": {
        "id": "tkg_QxQQ3To6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# soft accuracy:\n",
        "# parts of the code are taken from https://github.com/wildltr/ptranking, specifically \n",
        "# https://github.com/wildltr/ptranking/blob/master/ptranking/base/neural_utils.py\n",
        "\n",
        "# soft binning:\n",
        "# we have used parts of the code from https://github.com/wOOL/DNDT/blob/master/pytorch/demo.ipynb\n",
        "\n",
        "\n",
        "class Robust_Sigmoid(torch.autograd.Function):\n",
        "    ''' Aiming for a stable sigmoid operator with specified sigma '''\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, sigma=1.0, gpu=False):\n",
        "        '''\n",
        "        :param ctx:\n",
        "        :param input: the input tensor\n",
        "        :param sigma: the scaling constant\n",
        "        :return:\n",
        "        '''\n",
        "        x = input if 1.0 == sigma else sigma * input\n",
        "\n",
        "        torch_half = torch.cuda.FloatTensor(\n",
        "            [0.5]) if gpu else torch.FloatTensor([0.5])\n",
        "        sigmoid_x_pos = torch.where(\n",
        "            input > 0, 1./(1. + torch.exp(-x)), torch_half)\n",
        "\n",
        "        exp_x = torch.exp(x)\n",
        "        sigmoid_x = torch.where(input < 0, exp_x/(1.+exp_x), sigmoid_x_pos)\n",
        "\n",
        "        grad = sigmoid_x * \\\n",
        "            (1. - sigmoid_x) if 1.0 == sigma else sigma * \\\n",
        "            sigmoid_x * (1. - sigmoid_x)\n",
        "        ctx.save_for_backward(grad)\n",
        "\n",
        "        return sigmoid_x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        '''\n",
        "        :param ctx:\n",
        "        :param grad_output: backpropagated gradients from upper module(s)\n",
        "        :return:\n",
        "        '''\n",
        "        grad = ctx.saved_tensors[0]\n",
        "\n",
        "        bg = grad_output * grad  # chain rule\n",
        "\n",
        "        return bg, None, None\n",
        "\n",
        "\n",
        "#- function: robust_sigmoid-#\n",
        "robust_sigmoid = Robust_Sigmoid.apply"
      ],
      "metadata": {
        "id": "ogmUhhu_3Vvv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Framework(object):\n",
        "    def __init__(self, dataset_train:torch.utils.data.Dataset, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Basic Framework for your bayesian neural network.\n",
        "        Other solutions like MC Dropout, Ensemble learning will based upon this.\n",
        "        \"\"\"\n",
        "        self.train_set = dataset_train\n",
        "        self.print_interval = 100 # number of batches until updated metrics are displayed during training\n",
        "\n",
        "    def train(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def predict(self, data_loader: torch.utils.data.DataLoader) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Predict the class probabilities using your trained model.\n",
        "        This method should return an (num_samples, 10) NumPy float array\n",
        "        such that the second dimension sums up to 1 for each row.\n",
        "\n",
        "        :param data_loader: Data loader yielding the samples to predict on\n",
        "        :return: (num_samples, 10) NumPy float array where the second dimension sums up to 1 for each row\n",
        "        \"\"\"\n",
        "        probability_batches = []\n",
        "        \n",
        "        for batch_x, _ in tqdm.tqdm(data_loader):\n",
        "            current_probabilities = self.predict_probabilities(batch_x).detach().numpy()\n",
        "            probability_batches.append(current_probabilities)\n",
        "\n",
        "        output = np.concatenate(probability_batches, axis=0)\n",
        "        return output\n",
        "\n",
        "    def predict_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        raise NotImplementedError()"
      ],
      "metadata": {
        "id": "e08-Wi-erCZm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "dece = DECE(device, 30, 100, 0.01)"
      ],
      "metadata": {
        "id": "AtpJfjDE3buN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model:Framework, eval_loader: torch.utils.data.DataLoader, data_dir: str, output_dir: str):\n",
        "    \"\"\"\n",
        "    Evaluate your model.\n",
        "    :param model: Trained model to evaluate\n",
        "    :param eval_loader: Data loader containing the training set for evaluation\n",
        "    :param data_dir: Data directory from which additional datasets are loaded\n",
        "    :param output_dir: Directory into which plots are saved\n",
        "    \"\"\"\n",
        "    print(\"evaulating\")\n",
        "    # Predict class probabilities on test data\n",
        "    predicted_probabilities = model.predict(eval_loader)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
        "    actual_classes = eval_loader.dataset.tensors[1].detach().cpu().numpy()\n",
        "    accuracy = np.mean((predicted_classes == actual_classes)) \n",
        "    ece_score = ece(predicted_probabilities, actual_classes)\n",
        "    print(f'Accuracy: {accuracy.item():.3f}, ECE score: {ece_score:.3f}')\n",
        "    \n",
        "    # TODO: Reliability_diagram_3. draw reliability diagram on Test Data\n",
        "    # You can uncomment the below code to make it run. You can learn from\n",
        "    # the graph about how to improve your model. Remember first to complete \n",
        "    # the function of calc_calibration_curve.\n",
        "  \n",
        "    # print('Plotting reliability diagram on Test Dataset')\n",
        "    # out = calc_calibration_curve(predicted_probabilities, actual_classes, num_bins = 30)\n",
        "    # fig = draw_reliability_diagram(out)\n",
        "    # fig.savefig(os.path.join(output_dir, 'reliability-diagram.pdf'))\n",
        "    EXTENDED_EVALUATION = False\n",
        "    if EXTENDED_EVALUATION:\n",
        "        if not os.path.isdir(output_dir):\n",
        "            os.mkdir(output_dir)\n",
        "        ### draw reliability diagram\n",
        "        print('Plotting reliability diagram')\n",
        "        reliability_diagram_datax = np.load(os.path.join(data_dir, 'reliability_diagram_data_x.npy'))\n",
        "        reliability_diagram_datay = np.load(os.path.join(data_dir, 'reliability_diagram_data_y.npy'))\n",
        "        for i in range(3):\n",
        "            demo_prob_i = reliability_diagram_datax[i]\n",
        "            demo_label_i = reliability_diagram_datay[i]\n",
        "            out = calc_calibration_curve(demo_prob_i, demo_label_i, num_bins = 9)\n",
        "            fig = draw_reliability_diagram(out)\n",
        "            fig.savefig(os.path.join(output_dir, str(i)+'_reliability-diagram.pdf'))\n",
        "        \n",
        "        eval_samples = eval_loader.dataset.tensors[0].detach().cpu().numpy()\n",
        "\n",
        "        # Determine confidence per sample and sort accordingly\n",
        "        confidences = np.max(predicted_probabilities, axis=1)\n",
        "        sorted_confidence_indices = np.argsort(confidences)\n",
        "\n",
        "        # Plot samples your model is most confident about\n",
        "        print('Plotting most confident MNIST predictions')\n",
        "        most_confident_indices = sorted_confidence_indices[-10:]\n",
        "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
        "        for row in range(0, 4, 2):\n",
        "            for col in range(5):\n",
        "                sample_idx = most_confident_indices[5 * row // 2 + col]\n",
        "                ax[row, col].imshow(np.reshape(eval_samples[sample_idx], (28, 28)), cmap='gray')\n",
        "                ax[row, col].set_axis_off()\n",
        "                ax[row + 1, col].set_title(f'predicted {predicted_classes[sample_idx]}, actual {actual_classes[sample_idx]}')\n",
        "                bar_colors = ['C0'] * 10\n",
        "                bar_colors[actual_classes[sample_idx]] = 'C1'\n",
        "                ax[row + 1, col].bar(\n",
        "                    np.arange(10), predicted_probabilities[sample_idx], tick_label=np.arange(10), color=bar_colors\n",
        "                )\n",
        "        fig.suptitle('Most confident predictions', size=20)\n",
        "        fig.savefig(os.path.join(output_dir, 'mnist_most_confident.pdf'))\n",
        "\n",
        "        # Plot samples your model is least confident about\n",
        "        print('Plotting least confident MNIST predictions')\n",
        "        least_confident_indices = sorted_confidence_indices[:10]\n",
        "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
        "        for row in range(0, 4, 2):\n",
        "            for col in range(5):\n",
        "                sample_idx = least_confident_indices[5 * row // 2 + col]\n",
        "                ax[row, col].imshow(np.reshape(eval_samples[sample_idx], (28, 28)), cmap='gray')\n",
        "                ax[row, col].set_axis_off()\n",
        "                ax[row + 1, col].set_title(f'predicted {predicted_classes[sample_idx]}, actual {actual_classes[sample_idx]}')\n",
        "                bar_colors = ['C0'] * 10\n",
        "                bar_colors[actual_classes[sample_idx]] = 'C1'\n",
        "                ax[row + 1, col].bar(\n",
        "                    np.arange(10), predicted_probabilities[sample_idx], tick_label=np.arange(10), color=bar_colors\n",
        "                )\n",
        "        fig.suptitle('Least confident predictions', size=20)\n",
        "        fig.savefig(os.path.join(output_dir, 'mnist_least_confident.pdf'))\n",
        "\n",
        "        print('Plotting ambiguous and rotated MNIST confidences')\n",
        "        ambiguous_samples = torch.from_numpy(np.load(os.path.join(data_dir, 'test_x.npz'))['test_x']).reshape([-1, 784])[:10]\n",
        "        ambiguous_dataset = torch.utils.data.TensorDataset(ambiguous_samples, torch.zeros(10))\n",
        "        ambiguous_loader = torch.utils.data.DataLoader(\n",
        "            ambiguous_dataset, batch_size=10, shuffle=False, drop_last=False\n",
        "        )\n",
        "        ambiguous_predicted_probabilities = model.predict(ambiguous_loader)\n",
        "        ambiguous_predicted_classes = np.argmax(ambiguous_predicted_probabilities, axis=1)\n",
        "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
        "        for row in range(0, 4, 2):\n",
        "            for col in range(5):\n",
        "                sample_idx = 5 * row // 2 + col\n",
        "                ax[row, col].imshow(np.reshape(ambiguous_samples[sample_idx], (28, 28)), cmap='gray')\n",
        "                ax[row, col].set_axis_off()\n",
        "                ax[row + 1, col].set_title(f'predicted {ambiguous_predicted_classes[sample_idx]}')\n",
        "                ax[row + 1, col].bar(\n",
        "                    np.arange(10), ambiguous_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
        "                )\n",
        "        fig.suptitle('Predictions on ambiguous and rotated MNIST', size=20)\n",
        "        fig.savefig(os.path.join(output_dir, 'ambiguous_rotated_mnist.pdf'))\n",
        "\n",
        "\n",
        "        # Do the same evaluation as on MNIST also on FashionMNIST\n",
        "        print('Predicting on FashionMNIST data')\n",
        "        fmnist_samples = torch.from_numpy(np.load(os.path.join(data_dir, 'fmnist.npz'))['x_test'])#.reshape([-1, 784])\n",
        "        fmnist_dataset = torch.utils.data.TensorDataset(fmnist_samples, torch.zeros(fmnist_samples.shape[0]))\n",
        "        fmnist_loader = torch.utils.data.DataLoader(\n",
        "            fmnist_dataset, batch_size=64, shuffle=False, drop_last=False\n",
        "        )\n",
        "        fmnist_predicted_probabilities = model.predict(fmnist_loader)\n",
        "        fmnist_predicted_classes = np.argmax(fmnist_predicted_probabilities, axis=1)\n",
        "        fmnist_confidences = np.max(fmnist_predicted_probabilities, axis=1)\n",
        "        fmnist_sorted_confidence_indices = np.argsort(fmnist_confidences)\n",
        "\n",
        "        # Plot FashionMNIST samples your model is most confident about\n",
        "        print('Plotting most confident FashionMNIST predictions')\n",
        "        most_confident_indices = fmnist_sorted_confidence_indices[-10:]\n",
        "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
        "        for row in range(0, 4, 2):\n",
        "            for col in range(5):\n",
        "                sample_idx = most_confident_indices[5 * row // 2 + col]\n",
        "                ax[row, col].imshow(np.reshape(fmnist_samples[sample_idx], (28, 28)), cmap='gray')\n",
        "                ax[row, col].set_axis_off()\n",
        "                ax[row + 1, col].set_title(f'predicted {fmnist_predicted_classes[sample_idx]}')\n",
        "                ax[row + 1, col].bar(\n",
        "                    np.arange(10), fmnist_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
        "                )\n",
        "        fig.suptitle('Most confident predictions', size=20)\n",
        "        fig.savefig(os.path.join(output_dir, 'fashionmnist_most_confident.pdf'))\n",
        "\n",
        "        # Plot FashionMNIST samples your model is least confident about\n",
        "        print('Plotting least confident FashionMNIST predictions')\n",
        "        least_confident_indices = fmnist_sorted_confidence_indices[:10]\n",
        "        fig, ax = plt.subplots(4, 5, figsize=(13, 11))\n",
        "        for row in range(0, 4, 2):\n",
        "            for col in range(5):\n",
        "                sample_idx = least_confident_indices[5 * row // 2 + col]\n",
        "                ax[row, col].imshow(np.reshape(fmnist_samples[sample_idx], (28, 28)), cmap='gray')\n",
        "                ax[row, col].set_axis_off()\n",
        "                ax[row + 1, col].set_title(f'predicted {fmnist_predicted_classes[sample_idx]}')\n",
        "                ax[row + 1, col].bar(\n",
        "                    np.arange(10), fmnist_predicted_probabilities[sample_idx], tick_label=np.arange(10)\n",
        "                )\n",
        "        fig.suptitle('Least confident predictions', size=20)\n",
        "        fig.savefig(os.path.join(output_dir, 'fashionmnist_least_confident.pdf'))\n",
        "\n",
        "        print('Determining suitability of your model for OOD detection')\n",
        "        all_confidences = np.concatenate([confidences, fmnist_confidences])\n",
        "        dataset_labels = np.concatenate([np.ones_like(confidences), np.zeros_like(fmnist_confidences)])\n",
        "        print(\n",
        "            'AUROC for MNIST vs. FashionMNIST OOD detection based on confidence: '\n",
        "            f'{roc_auc_score(dataset_labels, all_confidences):.3f}'\n",
        "        )\n",
        "        print(\n",
        "            'AUPRC for MNIST vs. FashionMNIST OOD detection based on confidence: '\n",
        "            f'{average_precision_score(dataset_labels, all_confidences):.3f}'\n",
        "        )"
      ],
      "metadata": {
        "id": "u6io-A5urEe-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.LazyConv2d(8, 3, 1)\n",
        "        self.conv2 = nn.LazyConv2d(16, 3, 1)\n",
        "        self.fc1 = nn.LazyLinear(128)\n",
        "        self.fc2 = nn.LazyLinear(10)\n",
        "        self.d2d = nn.Dropout2d(p=0.1)\n",
        "        # self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        # x = F.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.dropout(x,training=True,p=0.3)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x,training=True,p=0.3)\n",
        "        x = self.fc2(x)\n",
        "        # output = self.softmax(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tsMiG0SNrGLd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DropoutTrainer(Framework):\n",
        "    def __init__(self, dataset_train,\n",
        "                 *args, **kwargs):\n",
        "        super().__init__(dataset_train, *args, **kwargs)\n",
        "\n",
        "        # Hyperparameters and general parameters\n",
        "        # TODO: MC_Dropout_4. Do experiments and tune hyperparameters\n",
        "        self.batch_size = 64\n",
        "        self.learning_rate = 1e-3\n",
        "        self.num_epochs = 100\n",
        "        torch.manual_seed(0) # set seed for reproducibility\n",
        "        \n",
        "        # TODO: MC_Dropout_1. Initialize the MC_Dropout network and optimizer here\n",
        "        # You can check the Dummy Trainer above for intuition about what to do\n",
        "        self.network = MNISTNet()\n",
        "        self.train_loader = DataLoader(\n",
        "            dataset_train, batch_size=self.batch_size, shuffle=True, drop_last=True\n",
        "            )\n",
        "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=self.learning_rate) \n",
        "        \n",
        "\n",
        "    def train(self):\n",
        "        self.network.train()\n",
        "        self.network  = self.network.to(\"cuda\")\n",
        "        # self.train_loader  = self.train_loader.to(\"cuda\")\n",
        "        progress_bar = trange(self.num_epochs)\n",
        "        # scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "        for epoch in progress_bar:\n",
        "            \n",
        "            if epoch % 50 == 0:\n",
        "                torch.save(self.network.state_dict(),f\"trainer_{epoch}.pth\")\n",
        "\n",
        "            for batch_idx, (batch_x, batch_y) in enumerate(self.train_loader):\n",
        "                # batch_x are of shape (batch_size, 784), batch_y are of shape (batch_size,)\n",
        "\n",
        "                batch_x = batch_x.to(\"cuda\").view(-1,1,28,28)\n",
        "                batch_y = batch_y.to(\"cuda\")\n",
        "\n",
        "                self.network.zero_grad()\n",
        "                # TODO: MC_Dropout_2. Implement MCDropout training here\n",
        "                # You need to calculate the loss based on the literature\n",
        "                preds = F.softmax(self.network(batch_x),dim=-1)\n",
        "                # loss = F.nll_loss(preds,batch_y)\n",
        "                ece = dece(preds, batch_y)\n",
        "                loss = F.cross_entropy(preds, batch_y)\n",
        "                loss = loss + ece*(epoch/np.sqrt(self.num_epochs))\n",
        "                # preds = preds.cpu()\n",
        "                # batch_y = batch_y.cpu()\n",
        "                # ece_ = ece(preds,batch_y)\n",
        "                # ece_ = ece_.to(\"cuda\")\n",
        "                # scheduler.step(val_loss)\n",
        "                # loss = loss + ece_\n",
        "                # loss = F.nll_loss(preds,batch_y)\n",
        "\n",
        "                # Backpropagate to get the gradients\n",
        "                loss.backward()\n",
        "\n",
        "                self.optimizer.step()\n",
        "                # Update progress bar with accuracy occasionally\n",
        "                if batch_idx % self.print_interval == 0:\n",
        "                    current_logits = self.network(batch_x)\n",
        "                    current_accuracy = (current_logits.argmax(axis=1) == batch_y).float().mean()\n",
        "                    progress_bar.set_postfix(loss=loss.item(), acc=current_accuracy.item())\n",
        "          \n",
        "\n",
        "    def predict_probabilities(self, x: torch.Tensor, num_sample=100) -> torch.Tensor:\n",
        "        self.network.eval()\n",
        "\n",
        "        x = x.to(\"cuda\").view(-1,1,28,28)\n",
        "        # TODO: MC_Dropout_3. Implement your MC_dropout prediction here\n",
        "        # You need to sample from your trained model here multiple times\n",
        "        # in order to implement Monte Carlo integration\n",
        "        preds = []\n",
        "        for i in range(10):\n",
        "            pred = self.network(x)\n",
        "            pred = F.softmax(pred,dim=-1)\n",
        "            pred = pred.detach().cpu().numpy()\n",
        "            preds.append(pred)\n",
        "\n",
        "        preds = np.array(preds)\n",
        "\n",
        "        # print(\"preds shape\")\n",
        "        # print(preds.shape)\n",
        "        # print(preds)\n",
        "\n",
        "        estimated_probability = preds.mean(axis=0)\n",
        "\n",
        "        # print(\"estimated_probability shape\")\n",
        "        # print(estimated_probability.shape)\n",
        "        # print(estimated_probability[0])\n",
        "\n",
        "        estimated_probability = torch.from_numpy(estimated_probability)\n",
        "        \n",
        "        assert estimated_probability.shape == (x.shape[0], 10)  \n",
        "        return estimated_probability"
      ],
      "metadata": {
        "id": "5nQuUof8rI8a"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "# soft accuracy:\n",
        "# parts of the code are taken from https://github.com/wildltr/ptranking, specifically \n",
        "# https://github.com/wildltr/ptranking/blob/master/ptranking/base/neural_utils.py\n",
        "\n",
        "# soft binning:\n",
        "# we have used parts of the code from https://github.com/wOOL/DNDT/blob/master/pytorch/demo.ipynb\n",
        "\n",
        "\n",
        "class Robust_Sigmoid(torch.autograd.Function):\n",
        "    ''' Aiming for a stable sigmoid operator with specified sigma '''\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, sigma=1.0, gpu=False):\n",
        "        '''\n",
        "        :param ctx:\n",
        "        :param input: the input tensor\n",
        "        :param sigma: the scaling constant\n",
        "        :return:\n",
        "        '''\n",
        "        x = input if 1.0 == sigma else sigma * input\n",
        "\n",
        "        torch_half = torch.cuda.FloatTensor(\n",
        "            [0.5]) if gpu else torch.FloatTensor([0.5])\n",
        "        sigmoid_x_pos = torch.where(\n",
        "            input > 0, 1./(1. + torch.exp(-x)), torch_half)\n",
        "\n",
        "        exp_x = torch.exp(x)\n",
        "        sigmoid_x = torch.where(input < 0, exp_x/(1.+exp_x), sigmoid_x_pos)\n",
        "\n",
        "        grad = sigmoid_x * \\\n",
        "            (1. - sigmoid_x) if 1.0 == sigma else sigma * \\\n",
        "            sigmoid_x * (1. - sigmoid_x)\n",
        "        ctx.save_for_backward(grad)\n",
        "\n",
        "        return sigmoid_x\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        '''\n",
        "        :param ctx:\n",
        "        :param grad_output: backpropagated gradients from upper module(s)\n",
        "        :return:\n",
        "        '''\n",
        "        grad = ctx.saved_tensors[0]\n",
        "\n",
        "        bg = grad_output * grad  # chain rule\n",
        "\n",
        "        return bg, None, None\n",
        "\n",
        "\n",
        "#- function: robust_sigmoid-#\n",
        "robust_sigmoid = Robust_Sigmoid.apply\n",
        "\n",
        "\n",
        "class DECE(nn.Module):\n",
        "    \"\"\"\n",
        "    Computes DECE loss (differentiable expected calibration error).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, device, num_bins, t_a, t_b):\n",
        "        super(DECE, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_bins = num_bins\n",
        "        self.t_a = t_a\n",
        "        self.t_b = t_b\n",
        "\n",
        "    def one_hot(self, indices, depth):\n",
        "        \"\"\"\n",
        "        Returns a one-hot tensor.\n",
        "        This is a PyTorch equivalent of Tensorflow's tf.one_hot.\n",
        "        Parameters:\n",
        "        indices:  a (n_batch, m) Tensor or (m) Tensor.\n",
        "        depth: a scalar. Represents the depth of the one hot dimension.\n",
        "        Returns: a (n_batch, m, depth) Tensor or (m, depth) Tensor.\n",
        "        \"\"\"\n",
        "        encoded_indicies = torch.zeros(\n",
        "            indices.size() + torch.Size([depth])).to(device=self.device)\n",
        "        index = indices.view(indices.size() + torch.Size([1]))\n",
        "        encoded_indicies = encoded_indicies.scatter_(1, index, 1)\n",
        "\n",
        "        return encoded_indicies\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        if input.dim() > 2:\n",
        "            # N,C,H,W => N,C,H*W\n",
        "            input = input.view(input.size(0), input.size(1), -1)\n",
        "            input = input.transpose(1, 2)    # N,C,H*W => N,H*W,C\n",
        "            input = input.contiguous().view(-1, input.size(2))   # N,H*W,C => N*H*W,C\n",
        "\n",
        "        # For CIFAR-10 and CIFAR-100, target.shape is [N] to begin with\n",
        "        target = target.view(-1)\n",
        "\n",
        "        predicted_probs = F.softmax(input, dim=1)\n",
        "\n",
        "        cut_points = torch.linspace(0, 1, self.num_bins + 1)[:-1].to(device=self.device)\n",
        "        W = torch.reshape(torch.linspace(1.0, self.num_bins, self.num_bins).to(device=self.device), [1, -1])\n",
        "        b = torch.cumsum(-cut_points, 0)\n",
        "\n",
        "        confidences = torch.max(predicted_probs, dim=1, keepdim=True)[0]\n",
        "        h = torch.matmul(confidences, W) + b\n",
        "        h = h / self.t_b\n",
        "\n",
        "        bin_probs = F.softmax(h, dim=1)\n",
        "\n",
        "        # smoothen the probabilities to avoid zeros\n",
        "        eps = 1e-6\n",
        "        bin_probs = bin_probs + eps\n",
        "        # normalize the probabilities to sum to one across bins\n",
        "        bin_probs = bin_probs / (1.0 + (self.num_bins + 1) * eps)\n",
        "\n",
        "        # calculate bin confidences\n",
        "        bin_confs = torch.div(bin_probs.transpose(0, 1).matmul(confidences).view(-1),\n",
        "                              torch.sum(bin_probs, dim=0))\n",
        "        # all-pairs approach\n",
        "        batch_pred_diffs = torch.unsqueeze(predicted_probs, dim=2) - torch.unsqueeze(predicted_probs, dim=1)\n",
        "        # computing pairwise differences, i.e., Sij or Sxy\n",
        "        if str(self.device) == 'cpu':\n",
        "            gpu = False\n",
        "        else:\n",
        "            gpu = True\n",
        "        # using {-1.0*} may lead to a poor performance when compared with the above way;\n",
        "        batch_indicators = robust_sigmoid(torch.transpose(batch_pred_diffs, dim0=1, dim1=2), self.t_a, gpu)\n",
        "\n",
        "        # get approximated rank positions, i.e., hat_pi(x)\n",
        "        ranks_all = torch.sum(batch_indicators, dim=2) + 0.5\n",
        "        # the ranks go from 1 to C, with 1 being the best rank\n",
        "        true_ranks = ranks_all[torch.arange(ranks_all.size(0)), target]\n",
        "        accs = F.relu(2.0 - true_ranks)\n",
        "        bin_accs = torch.div(bin_probs.transpose(0, 1).matmul(accs).view(-1),\n",
        "                                torch.sum(bin_probs, dim=0))\n",
        "\n",
        "        # calculate overall ECE for the whole batch\n",
        "        ece = torch.sum(torch.sum(bin_probs, dim=0) * torch.abs(bin_accs - bin_confs) / bin_probs.shape[0], dim=0)\n",
        "        return ece"
      ],
      "metadata": {
        "id": "2LZR_19RvoJa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_solution(dataset_train: torch.utils.data.Dataset, data_dir: str = os.curdir, output_dir: str = '/results/'):\n",
        "    \"\"\"\n",
        "    Run your task 2 solution.\n",
        "    This method should train your model, evaluate it, and return the trained model at the end.\n",
        "    Make sure to preserve the method signature and to return your trained model,\n",
        "    else the checker will fail!\n",
        "\n",
        "    :param dataset_train: Training dataset\n",
        "    :param data_dir: Directory containing the datasets\n",
        "    :return: Your trained model\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    trainer = DropoutTrainer(dataset_train=dataset_train)\n",
        "\n",
        "\n",
        "    # Train the model\n",
        "    print('Training model')\n",
        "    trainer.train()\n",
        "\n",
        "    # Predict using the trained model\n",
        "    print('Evaluating model on training data')\n",
        "    eval_loader = torch.utils.data.DataLoader(\n",
        "        dataset_train, batch_size=64, shuffle=False, drop_last=False\n",
        "    )\n",
        "    evaluate(trainer, eval_loader, data_dir, output_dir)\n",
        "\n",
        "    # IMPORTANT: return your model here!\n",
        "    return trainer"
      ],
      "metadata": {
        "id": "bFziV-Xvsv1q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "FU9oWLX_tLO-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load training data\n",
        "data_dir = os.curdir\n",
        "output_dir = os.curdir\n",
        "raw_train_data = np.load(os.path.join(data_dir, 'train_data.npz'))\n",
        "x_train = torch.from_numpy(raw_train_data['train_x']).to(device)\n",
        "y_train = torch.from_numpy(raw_train_data['train_y']).long().to(device)\n",
        "dataset_train = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "dataset_test = torch.utils.data.TensorDataset(x_train, y_train)\n",
        "# Run actual solution\n",
        "trainer = run_solution(dataset_train, data_dir=data_dir, output_dir=output_dir)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txZgPh8lsxS6",
        "outputId": "77e04813-d308-4edd-8d63-425ab3ba3aeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▍        | 14/100 [00:32<02:13,  1.56s/it, acc=0.406, loss=2.28]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = run_solution(dataset_train, data_dir=data_dir, output_dir=output_dir)"
      ],
      "metadata": {
        "id": "Yz0qhakQBlgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.network = trainer.network.cpu()\n",
        "torch.save(trainer.network.state_dict(),\"trainer.pth\")\n",
        "# torch.save(trainer.state_dict(), 'trainer.pth')"
      ],
      "metadata": {
        "id": "wwRdni3PsytX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ewmC9b2Zs0CB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}