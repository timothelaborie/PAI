{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "from sklearn.gaussian_process.kernels import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "from autograd import grad \n",
    "import tensorflow as tf\n",
    "from keras.losses import categorical_crossentropy, mean_squared_error\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras.metrics import AUC\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.activations import *\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import logging\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from flaml import AutoML\n",
    "\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function constants\n",
    "COST_W_UNDERPREDICT = 25.0\n",
    "COST_W_NORMAL = 1.0\n",
    "COST_W_OVERPREDICT = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(ground_truth: np.ndarray, predictions: np.ndarray) -> float:\n",
    "    # Unweighted cost\n",
    "    cost = (ground_truth - predictions) ** 2\n",
    "    weights = np.ones_like(cost) * COST_W_NORMAL\n",
    "\n",
    "    # Case i): underprediction\n",
    "    mask_1 = predictions < ground_truth\n",
    "    weights[mask_1] = COST_W_UNDERPREDICT\n",
    "\n",
    "    # Case ii): significant overprediction\n",
    "    mask_2 = (predictions >= 1.2*ground_truth)\n",
    "    weights[mask_2] = COST_W_OVERPREDICT\n",
    "\n",
    "    # Weigh the cost and return the average\n",
    "    return np.mean(cost * weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the cost function to a tensorflow loss function\n",
    "def cost_function_tf(): \n",
    "  def cost_fn(y_true, y_pred):\n",
    "    # Unweighted cost\n",
    "    cost = (y_true - y_pred) ** 2\n",
    "    weights = tf.ones_like(cost) * COST_W_NORMAL\n",
    "\n",
    "    # Case i): underprediction\n",
    "    mask_1 = tf.cast(y_pred < y_true, tf.float32)\n",
    "    weights = weights + mask_1 * (COST_W_UNDERPREDICT - COST_W_NORMAL)\n",
    "\n",
    "    # Case ii): significant overprediction\n",
    "    mask_2 = tf.cast(y_pred >= 1.2*y_true, tf.float32)\n",
    "    weights = weights + mask_2 * (COST_W_OVERPREDICT - COST_W_NORMAL)\n",
    "\n",
    "    # Weigh the cost and return the average\n",
    "    return tf.reduce_mean(cost * weights)\n",
    "    \n",
    "  return cost_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "95/95 [==============================] - 2s 12ms/step - loss: 5642.0737 - cost_fn: 5640.5356 - val_loss: 3731.8118 - val_cost_fn: 3731.9402\n",
      "Epoch 2/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 3793.6536 - cost_fn: 3793.0076 - val_loss: 3776.5408 - val_cost_fn: 3775.2312\n",
      "Epoch 3/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 3810.4836 - cost_fn: 3810.0168 - val_loss: 3941.7327 - val_cost_fn: 3941.8333\n",
      "Epoch 4/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 3749.4243 - cost_fn: 3749.0559 - val_loss: 3630.5850 - val_cost_fn: 3630.4121\n",
      "Epoch 5/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 3650.7690 - cost_fn: 3650.6648 - val_loss: 3521.0935 - val_cost_fn: 3520.1257\n",
      "Epoch 6/100\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 3457.3506 - cost_fn: 3457.2639 - val_loss: 3283.0530 - val_cost_fn: 3282.7283\n",
      "Epoch 7/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 3445.8853 - cost_fn: 3445.7307 - val_loss: 3134.8960 - val_cost_fn: 3134.2649\n",
      "Epoch 8/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 3162.3955 - cost_fn: 3162.3530 - val_loss: 2908.1467 - val_cost_fn: 2905.4414\n",
      "Epoch 9/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 2897.2776 - cost_fn: 2897.1968 - val_loss: 2647.0125 - val_cost_fn: 2647.6599\n",
      "Epoch 10/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 2680.1582 - cost_fn: 2679.9268 - val_loss: 2668.6829 - val_cost_fn: 2668.0454\n",
      "Epoch 11/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 2554.9006 - cost_fn: 2554.6912 - val_loss: 2550.9270 - val_cost_fn: 2549.4744\n",
      "Epoch 12/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 2322.7571 - cost_fn: 2322.4243 - val_loss: 2164.7590 - val_cost_fn: 2163.2764\n",
      "Epoch 13/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 2136.2795 - cost_fn: 2136.3396 - val_loss: 2034.4852 - val_cost_fn: 2033.7637\n",
      "Epoch 14/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 2036.7043 - cost_fn: 2036.8408 - val_loss: 1983.4448 - val_cost_fn: 1982.9508\n",
      "Epoch 15/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1917.6879 - cost_fn: 1918.2505 - val_loss: 1756.0406 - val_cost_fn: 1753.9399\n",
      "Epoch 16/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1917.1230 - cost_fn: 1917.1036 - val_loss: 1794.8983 - val_cost_fn: 1791.8317\n",
      "Epoch 17/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1786.7733 - cost_fn: 1786.9237 - val_loss: 1739.1451 - val_cost_fn: 1738.1147\n",
      "Epoch 18/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1744.0271 - cost_fn: 1744.0024 - val_loss: 1761.3667 - val_cost_fn: 1762.1670\n",
      "Epoch 19/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1668.8500 - cost_fn: 1669.0079 - val_loss: 1513.3491 - val_cost_fn: 1510.6356\n",
      "Epoch 20/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1545.6617 - cost_fn: 1545.6410 - val_loss: 1685.7083 - val_cost_fn: 1684.9293\n",
      "Epoch 21/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1522.8336 - cost_fn: 1522.9370 - val_loss: 1334.1934 - val_cost_fn: 1332.1428\n",
      "Epoch 22/100\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1415.0944 - cost_fn: 1414.9762 - val_loss: 1273.3063 - val_cost_fn: 1269.3409\n",
      "Epoch 23/100\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1343.9500 - cost_fn: 1343.5944 - val_loss: 1216.1030 - val_cost_fn: 1213.0411\n",
      "Epoch 24/100\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1293.4114 - cost_fn: 1293.3306 - val_loss: 1397.0936 - val_cost_fn: 1391.6381\n",
      "Epoch 25/100\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1309.8401 - cost_fn: 1309.8940 - val_loss: 1245.4723 - val_cost_fn: 1240.8265\n",
      "Epoch 26/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1232.5048 - cost_fn: 1232.6373 - val_loss: 1366.1005 - val_cost_fn: 1362.5133\n",
      "Epoch 27/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1103.1858 - cost_fn: 1102.9336 - val_loss: 1151.0382 - val_cost_fn: 1149.2069\n",
      "Epoch 28/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1134.4613 - cost_fn: 1134.3015 - val_loss: 1171.9744 - val_cost_fn: 1170.4196\n",
      "Epoch 29/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1128.8439 - cost_fn: 1128.7141 - val_loss: 1108.9847 - val_cost_fn: 1107.3370\n",
      "Epoch 30/100\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 1088.4733 - cost_fn: 1088.5991 - val_loss: 1270.5027 - val_cost_fn: 1268.5465\n",
      "Epoch 31/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 1187.9303 - cost_fn: 1187.8613 - val_loss: 1220.4156 - val_cost_fn: 1218.8390\n",
      "Epoch 32/100\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 987.5067 - cost_fn: 987.6117 - val_loss: 990.7237 - val_cost_fn: 989.2164\n",
      "Epoch 33/100\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 983.0104 - cost_fn: 982.9008 - val_loss: 809.4861 - val_cost_fn: 808.6053\n",
      "Epoch 34/100\n",
      "95/95 [==============================] - 1s 12ms/step - loss: 922.5277 - cost_fn: 922.7486 - val_loss: 955.4520 - val_cost_fn: 954.0235\n",
      "Epoch 35/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 929.4650 - cost_fn: 929.5052 - val_loss: 853.0775 - val_cost_fn: 852.4127\n",
      "Epoch 36/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 908.8478 - cost_fn: 908.7936 - val_loss: 824.3575 - val_cost_fn: 821.8232\n",
      "Epoch 37/100\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 936.5327 - cost_fn: 936.5706 - val_loss: 988.5126 - val_cost_fn: 990.1521\n",
      "Epoch 38/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 796.2251 - cost_fn: 796.3663 - val_loss: 822.9741 - val_cost_fn: 822.5004\n",
      "Epoch 39/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 843.0184 - cost_fn: 842.9863 - val_loss: 750.0925 - val_cost_fn: 748.6838\n",
      "Epoch 40/100\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 714.9802 - cost_fn: 715.0524 - val_loss: 727.9449 - val_cost_fn: 727.2819\n",
      "Epoch 41/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 735.4000 - cost_fn: 735.1799 - val_loss: 890.2397 - val_cost_fn: 890.9241\n",
      "Epoch 42/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 794.1428 - cost_fn: 794.0334 - val_loss: 774.1770 - val_cost_fn: 775.0975\n",
      "Epoch 43/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 823.2324 - cost_fn: 823.1871 - val_loss: 810.7847 - val_cost_fn: 808.9672\n",
      "Epoch 44/100\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 770.9343 - cost_fn: 770.8186 - val_loss: 722.5127 - val_cost_fn: 721.7160\n",
      "Epoch 45/100\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 881.1604 - cost_fn: 881.0037 - val_loss: 711.9707 - val_cost_fn: 709.7060\n",
      "Epoch 46/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 659.3104 - cost_fn: 659.3953 - val_loss: 671.3522 - val_cost_fn: 671.6423\n",
      "Epoch 47/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 772.0426 - cost_fn: 772.1887 - val_loss: 905.0801 - val_cost_fn: 904.5757\n",
      "Epoch 48/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 666.1631 - cost_fn: 666.1827 - val_loss: 523.5967 - val_cost_fn: 522.7314\n",
      "Epoch 49/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 603.2789 - cost_fn: 603.2715 - val_loss: 678.2178 - val_cost_fn: 678.0602\n",
      "Epoch 50/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 613.9501 - cost_fn: 613.8761 - val_loss: 621.1873 - val_cost_fn: 620.9492\n",
      "Epoch 51/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 693.0824 - cost_fn: 692.9721 - val_loss: 822.6065 - val_cost_fn: 822.6370\n",
      "Epoch 52/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 652.5523 - cost_fn: 652.5544 - val_loss: 788.7928 - val_cost_fn: 789.2819\n",
      "Epoch 53/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 558.0803 - cost_fn: 558.0016 - val_loss: 629.7878 - val_cost_fn: 629.0562\n",
      "Epoch 54/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 562.4229 - cost_fn: 562.6476 - val_loss: 790.1650 - val_cost_fn: 788.5958\n",
      "Epoch 55/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 600.6644 - cost_fn: 600.6917 - val_loss: 683.6060 - val_cost_fn: 682.3741\n",
      "Epoch 56/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 568.8547 - cost_fn: 568.7385 - val_loss: 552.8215 - val_cost_fn: 554.4542\n",
      "Epoch 57/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 523.5897 - cost_fn: 523.7416 - val_loss: 634.1008 - val_cost_fn: 633.9802\n",
      "Epoch 58/100\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 482.0202 - cost_fn: 481.9885 - val_loss: 504.1407 - val_cost_fn: 503.4989\n",
      "Epoch 59/100\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 588.4272 - cost_fn: 588.4139 - val_loss: 693.9551 - val_cost_fn: 693.6162\n",
      "Epoch 60/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 587.6672 - cost_fn: 587.6428 - val_loss: 593.4036 - val_cost_fn: 593.3575\n",
      "Epoch 61/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 474.1134 - cost_fn: 473.9853 - val_loss: 679.7493 - val_cost_fn: 680.2045\n",
      "Epoch 62/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 535.0726 - cost_fn: 535.1185 - val_loss: 818.5368 - val_cost_fn: 817.6931\n",
      "Epoch 63/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 478.1879 - cost_fn: 478.2408 - val_loss: 423.3211 - val_cost_fn: 423.7769\n",
      "Epoch 64/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 467.5001 - cost_fn: 467.5099 - val_loss: 454.5486 - val_cost_fn: 454.2183\n",
      "Epoch 65/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 491.7091 - cost_fn: 491.7846 - val_loss: 462.3341 - val_cost_fn: 461.6343\n",
      "Epoch 66/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 523.5369 - cost_fn: 523.6080 - val_loss: 528.5453 - val_cost_fn: 528.9908\n",
      "Epoch 67/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 523.3736 - cost_fn: 523.4383 - val_loss: 524.4783 - val_cost_fn: 524.6743\n",
      "Epoch 68/100\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 481.4970 - cost_fn: 481.3537 - val_loss: 507.0406 - val_cost_fn: 507.3264\n",
      "Epoch 69/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 604.5731 - cost_fn: 604.4603 - val_loss: 520.1406 - val_cost_fn: 520.0182\n",
      "Epoch 70/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 430.2436 - cost_fn: 430.2111 - val_loss: 372.9374 - val_cost_fn: 373.3654\n",
      "Epoch 71/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 512.1476 - cost_fn: 511.9835 - val_loss: 363.7403 - val_cost_fn: 363.2601\n",
      "Epoch 72/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 411.1943 - cost_fn: 411.2037 - val_loss: 498.2941 - val_cost_fn: 498.4330\n",
      "Epoch 73/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 362.9029 - cost_fn: 362.9954 - val_loss: 414.1711 - val_cost_fn: 414.2157\n",
      "Epoch 74/100\n",
      "95/95 [==============================] - 1s 11ms/step - loss: 446.9956 - cost_fn: 446.9964 - val_loss: 475.0817 - val_cost_fn: 475.4904\n",
      "Epoch 75/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 508.3437 - cost_fn: 508.2993 - val_loss: 442.9700 - val_cost_fn: 442.5672\n",
      "Epoch 76/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 374.5516 - cost_fn: 374.5256 - val_loss: 330.8746 - val_cost_fn: 330.9377\n",
      "Epoch 77/100\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 375.8252 - cost_fn: 375.9215 - val_loss: 472.7977 - val_cost_fn: 471.7888\n",
      "Epoch 78/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 425.5565 - cost_fn: 425.6057 - val_loss: 469.9919 - val_cost_fn: 468.7483\n",
      "Epoch 79/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 425.2963 - cost_fn: 425.3395 - val_loss: 532.6506 - val_cost_fn: 533.6218\n",
      "Epoch 80/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 361.2503 - cost_fn: 361.2180 - val_loss: 404.7274 - val_cost_fn: 404.8730\n",
      "Epoch 81/100\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 381.8037 - cost_fn: 381.7654 - val_loss: 416.8703 - val_cost_fn: 416.4232\n",
      "Epoch 82/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 411.1339 - cost_fn: 411.0561 - val_loss: 361.0712 - val_cost_fn: 360.9728\n",
      "Epoch 83/100\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 365.4551 - cost_fn: 365.3982 - val_loss: 376.0954 - val_cost_fn: 376.3163\n",
      "Epoch 84/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 454.5966 - cost_fn: 454.5355 - val_loss: 397.9734 - val_cost_fn: 398.6502\n",
      "Epoch 85/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 395.4708 - cost_fn: 395.4442 - val_loss: 347.1340 - val_cost_fn: 346.8439\n",
      "Epoch 86/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 422.8341 - cost_fn: 422.7864 - val_loss: 485.0496 - val_cost_fn: 485.7622\n",
      "Epoch 87/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 464.2073 - cost_fn: 464.1809 - val_loss: 380.8933 - val_cost_fn: 380.6236\n",
      "Epoch 88/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 336.0885 - cost_fn: 336.1062 - val_loss: 263.2440 - val_cost_fn: 263.3580\n",
      "Epoch 89/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 349.6404 - cost_fn: 349.6077 - val_loss: 434.3101 - val_cost_fn: 434.4266\n",
      "Epoch 90/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 336.7592 - cost_fn: 336.7126 - val_loss: 403.3427 - val_cost_fn: 403.8777\n",
      "Epoch 91/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 307.2170 - cost_fn: 307.1616 - val_loss: 344.1058 - val_cost_fn: 344.1279\n",
      "Epoch 92/100\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 336.9549 - cost_fn: 336.9612 - val_loss: 312.3842 - val_cost_fn: 311.9880\n",
      "Epoch 93/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 360.6643 - cost_fn: 360.8613 - val_loss: 411.2580 - val_cost_fn: 410.6004\n",
      "Epoch 94/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 373.2168 - cost_fn: 373.2196 - val_loss: 506.4244 - val_cost_fn: 506.2790\n",
      "Epoch 95/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 373.9537 - cost_fn: 374.1020 - val_loss: 430.2523 - val_cost_fn: 428.9608\n",
      "Epoch 96/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 339.0127 - cost_fn: 339.0266 - val_loss: 311.0298 - val_cost_fn: 310.6738\n",
      "Epoch 97/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 323.9212 - cost_fn: 323.8773 - val_loss: 345.8438 - val_cost_fn: 345.8383\n",
      "Epoch 98/100\n",
      "95/95 [==============================] - 1s 9ms/step - loss: 329.3371 - cost_fn: 329.3731 - val_loss: 371.1219 - val_cost_fn: 371.7488\n",
      "Epoch 99/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 331.6164 - cost_fn: 331.6441 - val_loss: 379.6704 - val_cost_fn: 380.5362\n",
      "Epoch 100/100\n",
      "95/95 [==============================] - 1s 10ms/step - loss: 369.9849 - cost_fn: 369.9264 - val_loss: 362.9108 - val_cost_fn: 361.9398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2489e95e520>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training dateset and test features\n",
    "X = np.loadtxt('train_x.csv', delimiter=',', skiprows=1)\n",
    "y = np.loadtxt('train_y.csv', delimiter=',', skiprows=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Input(shape=(X.shape[1],)),\n",
    "    layers.Dense(2048, activation=relu),\n",
    "    # layers.Dropout(0.1),\n",
    "    layers.Dense(2048, activation=relu),\n",
    "    layers.Dense(2048, activation=relu),\n",
    "    layers.Dense(2048, activation=relu),\n",
    "    layers.Dense(2048, activation=relu),\n",
    "    layers.Dense(1, activation='linear')\n",
    "])\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=cost_function_tf(), metrics=[cost_function_tf()])\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_test, y_test), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
