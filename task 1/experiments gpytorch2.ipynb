{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "from sklearn.gaussian_process.kernels import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "from autograd import grad \n",
    "import tensorflow as tf\n",
    "from keras.losses import categorical_crossentropy, mean_squared_error\n",
    "from keras import layers\n",
    "from keras.layers import *\n",
    "from keras.metrics import AUC\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.activations import *\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import logging\n",
    "import time\n",
    "\n",
    "import os\n",
    "import typing\n",
    "from sklearn.gaussian_process.kernels import *\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import gpytorch\n",
    "import torch\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function constants\n",
    "COST_W_UNDERPREDICT = 25.0\n",
    "COST_W_NORMAL = 1.0\n",
    "COST_W_OVERPREDICT = 10.0\n",
    "def cost_function(ground_truth: np.ndarray, predictions: np.ndarray) -> float:\n",
    "    # Unweighted cost\n",
    "    cost = (ground_truth - predictions) ** 2\n",
    "    weights = np.ones_like(cost) * COST_W_NORMAL\n",
    "\n",
    "    # Case i): underprediction\n",
    "    mask_1 = predictions < ground_truth\n",
    "    weights[mask_1] = COST_W_UNDERPREDICT\n",
    "\n",
    "    # Case ii): significant overprediction\n",
    "    mask_2 = (predictions >= 1.2*ground_truth)\n",
    "    weights[mask_2] = COST_W_OVERPREDICT\n",
    "\n",
    "    # Weigh the cost and return the average\n",
    "    return np.mean(cost * weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Load the training dateset and test features\n",
    "x = np.loadtxt('train_x.csv', delimiter=',', skiprows=1)\n",
    "testx = np.loadtxt('test_x.csv', delimiter=',', skiprows=1)\n",
    "y = np.loadtxt('train_y.csv', delimiter=',', skiprows=1)\n",
    "largest = y.max()\n",
    "largest = 1\n",
    "print(largest)\n",
    "y/=largest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "# testx = torch.tensor(testx, dtype=torch.float32).to(device)\n",
    "testx = torch.tensor(testx, dtype=torch.float32).to(\"cpu\")\n",
    "y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(x, y, likelihood)\n",
    "\n",
    "\n",
    "# initialize lengthscale, outputscale and noise to random values between 0 and 0.1\n",
    "# model.covar_module.base_kernel.lengthscale = np.random.rand(1)*0.1\n",
    "# model.covar_module.outputscale = np.random.rand(1)*0.1\n",
    "# likelihood.noise = np.random.rand(1)*0.01\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "likelihood2 = gpytorch.likelihoods.GaussianLikelihood()\n",
    "likelihood2.eval()\n",
    "model2 = ExactGPModel(x, y, likelihood2)\n",
    "model2 = model2.to('cpu')\n",
    "model2.eval()\n",
    "\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# load the model\n",
    "# model.load_state_dict(torch.load('model_state.pth'))\n",
    "# model.load_state_dict(torch.load('model_state_improved6.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(gp_mean,gp_std):\n",
    "    params = {'gp_mean': gp_mean.tolist(), 'gp_std': gp_std.tolist()}\n",
    "    with open('predictions.json', 'w') as f:\n",
    "        json.dump(params, f)\n",
    "    # o = os.popen('docker build --tag task1 .').read()\n",
    "    o = os.popen('docker run --rm -v \"%cd%:/results\" task1').read()\n",
    "    o = o.replace(\"Congratulations, you have passed the checks on public dataset. Your cost is \",\"\").replace(\"Dumped check file to /results/results_check.byte\",\"\").replace(\"\\n\",\"\")\n",
    "    if \"improve\" in o:\n",
    "        return 99999\n",
    "    else:\n",
    "        return float(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0/5000 - Loss: 171.468 - Lengthscale: 0.693 - Noise: 0.693 - Outputscale: 0.693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\gpytorch\\distributions\\multivariate_normal.py:259: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0/5000 - Score: 99999.000\n",
      "Iter 100/5000 - Loss: 3.763 - Lengthscale: 0.021 - Noise: 2.571 - Outputscale: 5.871\n",
      "Iter 100/5000 - Score: 52.180\n",
      "Best model saved with score: 52.180\n",
      "Iter 200/5000 - Loss: 3.154 - Lengthscale: 0.018 - Noise: 2.603 - Outputscale: 7.808\n",
      "Iter 200/5000 - Score: 34.516\n",
      "Best model saved with score: 34.516\n",
      "Iter 300/5000 - Loss: 2.917 - Lengthscale: 0.017 - Noise: 2.622 - Outputscale: 9.258\n",
      "Iter 300/5000 - Score: 27.379\n",
      "Best model saved with score: 27.379\n",
      "Iter 400/5000 - Loss: 2.805 - Lengthscale: 0.017 - Noise: 2.633 - Outputscale: 10.508\n",
      "Iter 400/5000 - Score: 25.439\n",
      "Best model saved with score: 25.439\n",
      "Iter 500/5000 - Loss: 2.737 - Lengthscale: 0.016 - Noise: 2.637 - Outputscale: 11.657\n",
      "Iter 500/5000 - Score: 23.180\n",
      "Best model saved with score: 23.180\n",
      "Iter 600/5000 - Loss: 2.688 - Lengthscale: 0.016 - Noise: 2.636 - Outputscale: 12.741\n",
      "Iter 600/5000 - Score: 22.459\n",
      "Best model saved with score: 22.459\n",
      "Iter 700/5000 - Loss: 2.647 - Lengthscale: 0.016 - Noise: 2.629 - Outputscale: 13.776\n",
      "Iter 700/5000 - Score: 21.781\n",
      "Best model saved with score: 21.781\n",
      "Iter 800/5000 - Loss: 2.615 - Lengthscale: 0.016 - Noise: 2.617 - Outputscale: 14.770\n",
      "Iter 800/5000 - Score: 20.864\n",
      "Best model saved with score: 20.864\n",
      "Iter 825/5000 - Loss: 2.607 - Lengthscale: 0.016 - Noise: 2.613 - Outputscale: 15.013\n",
      "Iter 825/5000 - Score: 20.641\n",
      "Best model saved with score: 20.641\n",
      "Iter 850/5000 - Loss: 2.602 - Lengthscale: 0.016 - Noise: 2.609 - Outputscale: 15.254\n",
      "Iter 850/5000 - Score: 20.993\n",
      "Iter 875/5000 - Loss: 2.591 - Lengthscale: 0.016 - Noise: 2.604 - Outputscale: 15.494\n",
      "Iter 875/5000 - Score: 20.511\n",
      "Best model saved with score: 20.511\n",
      "Iter 900/5000 - Loss: 2.588 - Lengthscale: 0.016 - Noise: 2.599 - Outputscale: 15.732\n",
      "Iter 900/5000 - Score: 20.377\n",
      "Best model saved with score: 20.377\n",
      "Iter 925/5000 - Loss: 2.578 - Lengthscale: 0.016 - Noise: 2.594 - Outputscale: 15.968\n",
      "Iter 925/5000 - Score: 20.236\n",
      "Best model saved with score: 20.236\n",
      "Iter 950/5000 - Loss: 2.578 - Lengthscale: 0.016 - Noise: 2.588 - Outputscale: 16.202\n",
      "Iter 950/5000 - Score: 20.464\n",
      "Iter 975/5000 - Loss: 2.569 - Lengthscale: 0.016 - Noise: 2.582 - Outputscale: 16.435\n",
      "Iter 975/5000 - Score: 20.439\n",
      "Iter 1000/5000 - Loss: 2.565 - Lengthscale: 0.015 - Noise: 2.576 - Outputscale: 16.667\n",
      "Iter 1000/5000 - Score: 19.990\n",
      "Best model saved with score: 19.990\n",
      "Iter 1025/5000 - Loss: 2.558 - Lengthscale: 0.015 - Noise: 2.569 - Outputscale: 16.897\n",
      "Iter 1025/5000 - Score: 20.287\n",
      "Iter 1050/5000 - Loss: 2.551 - Lengthscale: 0.015 - Noise: 2.562 - Outputscale: 17.126\n",
      "Iter 1050/5000 - Score: 20.097\n",
      "Iter 1075/5000 - Loss: 2.546 - Lengthscale: 0.015 - Noise: 2.555 - Outputscale: 17.353\n",
      "Iter 1075/5000 - Score: 19.886\n",
      "Best model saved with score: 19.886\n",
      "Iter 1100/5000 - Loss: 2.543 - Lengthscale: 0.015 - Noise: 2.547 - Outputscale: 17.580\n",
      "Iter 1100/5000 - Score: 19.732\n",
      "Best model saved with score: 19.732\n",
      "Iter 1125/5000 - Loss: 2.536 - Lengthscale: 0.015 - Noise: 2.539 - Outputscale: 17.805\n",
      "Iter 1125/5000 - Score: 19.440\n",
      "Best model saved with score: 19.440\n",
      "Iter 1150/5000 - Loss: 2.529 - Lengthscale: 0.015 - Noise: 2.530 - Outputscale: 18.029\n",
      "Iter 1150/5000 - Score: 19.611\n",
      "Iter 1175/5000 - Loss: 2.525 - Lengthscale: 0.015 - Noise: 2.521 - Outputscale: 18.253\n",
      "Iter 1175/5000 - Score: 19.578\n",
      "Iter 1200/5000 - Loss: 2.522 - Lengthscale: 0.015 - Noise: 2.512 - Outputscale: 18.475\n",
      "Iter 1200/5000 - Score: 19.231\n",
      "Best model saved with score: 19.231\n",
      "Iter 1225/5000 - Loss: 2.524 - Lengthscale: 0.015 - Noise: 2.503 - Outputscale: 18.696\n",
      "Iter 1225/5000 - Score: 19.159\n",
      "Best model saved with score: 19.159\n",
      "Iter 1250/5000 - Loss: 2.513 - Lengthscale: 0.015 - Noise: 2.493 - Outputscale: 18.917\n",
      "Iter 1250/5000 - Score: 19.065\n",
      "Best model saved with score: 19.065\n",
      "Iter 1275/5000 - Loss: 2.508 - Lengthscale: 0.015 - Noise: 2.483 - Outputscale: 19.137\n",
      "Iter 1275/5000 - Score: 18.935\n",
      "Best model saved with score: 18.935\n",
      "Iter 1300/5000 - Loss: 2.505 - Lengthscale: 0.015 - Noise: 2.472 - Outputscale: 19.356\n",
      "Iter 1300/5000 - Score: 18.999\n",
      "Iter 1325/5000 - Loss: 2.505 - Lengthscale: 0.015 - Noise: 2.461 - Outputscale: 19.574\n",
      "Iter 1325/5000 - Score: 19.556\n",
      "Iter 1350/5000 - Loss: 2.503 - Lengthscale: 0.015 - Noise: 2.450 - Outputscale: 19.792\n",
      "Iter 1350/5000 - Score: 19.308\n",
      "Iter 1375/5000 - Loss: 2.495 - Lengthscale: 0.015 - Noise: 2.438 - Outputscale: 20.009\n",
      "Iter 1375/5000 - Score: 19.394\n",
      "Iter 1400/5000 - Loss: 2.495 - Lengthscale: 0.015 - Noise: 2.426 - Outputscale: 20.225\n",
      "Iter 1400/5000 - Score: 19.066\n",
      "Iter 1425/5000 - Loss: 2.491 - Lengthscale: 0.015 - Noise: 2.414 - Outputscale: 20.441\n",
      "Iter 1425/5000 - Score: 18.636\n",
      "Best model saved with score: 18.636\n",
      "Iter 1450/5000 - Loss: 2.489 - Lengthscale: 0.015 - Noise: 2.402 - Outputscale: 20.657\n",
      "Iter 1450/5000 - Score: 18.670\n",
      "Iter 1475/5000 - Loss: 2.484 - Lengthscale: 0.015 - Noise: 2.389 - Outputscale: 20.872\n",
      "Iter 1475/5000 - Score: 18.675\n",
      "Iter 1500/5000 - Loss: 2.477 - Lengthscale: 0.015 - Noise: 2.375 - Outputscale: 21.086\n",
      "Iter 1500/5000 - Score: 18.760\n",
      "Iter 1525/5000 - Loss: 2.481 - Lengthscale: 0.015 - Noise: 2.362 - Outputscale: 21.301\n",
      "Iter 1525/5000 - Score: 18.646\n",
      "Iter 1550/5000 - Loss: 2.472 - Lengthscale: 0.015 - Noise: 2.348 - Outputscale: 21.515\n",
      "Iter 1550/5000 - Score: 18.569\n",
      "Best model saved with score: 18.569\n",
      "Iter 1575/5000 - Loss: 2.470 - Lengthscale: 0.015 - Noise: 2.333 - Outputscale: 21.728\n",
      "Iter 1575/5000 - Score: 18.189\n",
      "Best model saved with score: 18.189\n",
      "Iter 1600/5000 - Loss: 2.465 - Lengthscale: 0.015 - Noise: 2.319 - Outputscale: 21.942\n",
      "Iter 1600/5000 - Score: 18.204\n",
      "Iter 1625/5000 - Loss: 2.465 - Lengthscale: 0.015 - Noise: 2.304 - Outputscale: 22.154\n",
      "Iter 1625/5000 - Score: 18.472\n",
      "Iter 1650/5000 - Loss: 2.463 - Lengthscale: 0.015 - Noise: 2.289 - Outputscale: 22.367\n",
      "Iter 1650/5000 - Score: 18.345\n",
      "Iter 1675/5000 - Loss: 2.456 - Lengthscale: 0.015 - Noise: 2.273 - Outputscale: 22.580\n",
      "Iter 1675/5000 - Score: 18.795\n",
      "Iter 1700/5000 - Loss: 2.454 - Lengthscale: 0.014 - Noise: 2.257 - Outputscale: 22.793\n",
      "Iter 1700/5000 - Score: 18.475\n",
      "Iter 1725/5000 - Loss: 2.450 - Lengthscale: 0.014 - Noise: 2.241 - Outputscale: 23.005\n",
      "Iter 1725/5000 - Score: 18.273\n",
      "Iter 1750/5000 - Loss: 2.446 - Lengthscale: 0.014 - Noise: 2.224 - Outputscale: 23.218\n",
      "Iter 1750/5000 - Score: 18.381\n",
      "Iter 1775/5000 - Loss: 2.451 - Lengthscale: 0.014 - Noise: 2.207 - Outputscale: 23.430\n",
      "Iter 1775/5000 - Score: 18.476\n",
      "Iter 1800/5000 - Loss: 2.440 - Lengthscale: 0.014 - Noise: 2.190 - Outputscale: 23.643\n",
      "Iter 1800/5000 - Score: 17.972\n",
      "Best model saved with score: 17.972\n",
      "Iter 1825/5000 - Loss: 2.441 - Lengthscale: 0.014 - Noise: 2.173 - Outputscale: 23.856\n",
      "Iter 1825/5000 - Score: 18.017\n",
      "Iter 1850/5000 - Loss: 2.439 - Lengthscale: 0.014 - Noise: 2.155 - Outputscale: 24.068\n",
      "Iter 1850/5000 - Score: 18.162\n",
      "Iter 1875/5000 - Loss: 2.443 - Lengthscale: 0.014 - Noise: 2.137 - Outputscale: 24.281\n",
      "Iter 1875/5000 - Score: 17.977\n",
      "Iter 1900/5000 - Loss: 2.438 - Lengthscale: 0.014 - Noise: 2.119 - Outputscale: 24.493\n",
      "Iter 1900/5000 - Score: 18.090\n",
      "Iter 1925/5000 - Loss: 2.437 - Lengthscale: 0.014 - Noise: 2.100 - Outputscale: 24.706\n",
      "Iter 1925/5000 - Score: 17.950\n",
      "Best model saved with score: 17.950\n",
      "Iter 1950/5000 - Loss: 2.434 - Lengthscale: 0.014 - Noise: 2.081 - Outputscale: 24.919\n",
      "Iter 1950/5000 - Score: 17.965\n",
      "Iter 1975/5000 - Loss: 2.426 - Lengthscale: 0.014 - Noise: 2.062 - Outputscale: 25.133\n",
      "Iter 1975/5000 - Score: 17.950\n",
      "Iter 2000/5000 - Loss: 2.430 - Lengthscale: 0.014 - Noise: 2.043 - Outputscale: 25.346\n",
      "Iter 2000/5000 - Score: 18.130\n",
      "Iter 2025/5000 - Loss: 2.425 - Lengthscale: 0.014 - Noise: 2.024 - Outputscale: 25.560\n",
      "Iter 2025/5000 - Score: 17.993\n",
      "Iter 2050/5000 - Loss: 2.420 - Lengthscale: 0.014 - Noise: 2.004 - Outputscale: 25.775\n",
      "Iter 2050/5000 - Score: 18.169\n",
      "Iter 2075/5000 - Loss: 2.423 - Lengthscale: 0.014 - Noise: 1.984 - Outputscale: 25.990\n",
      "Iter 2075/5000 - Score: 18.064\n",
      "Iter 2100/5000 - Loss: 2.418 - Lengthscale: 0.014 - Noise: 1.963 - Outputscale: 26.205\n",
      "Iter 2100/5000 - Score: 18.006\n",
      "Iter 2125/5000 - Loss: 2.415 - Lengthscale: 0.014 - Noise: 1.943 - Outputscale: 26.420\n",
      "Iter 2125/5000 - Score: 17.883\n",
      "Best model saved with score: 17.883\n",
      "Iter 2150/5000 - Loss: 2.412 - Lengthscale: 0.014 - Noise: 1.922 - Outputscale: 26.637\n",
      "Iter 2150/5000 - Score: 17.954\n",
      "Iter 2175/5000 - Loss: 2.413 - Lengthscale: 0.014 - Noise: 1.901 - Outputscale: 26.854\n",
      "Iter 2175/5000 - Score: 18.132\n",
      "Iter 2200/5000 - Loss: 2.408 - Lengthscale: 0.014 - Noise: 1.880 - Outputscale: 27.071\n",
      "Iter 2200/5000 - Score: 19.047\n",
      "Iter 2225/5000 - Loss: 2.411 - Lengthscale: 0.014 - Noise: 1.858 - Outputscale: 27.289\n",
      "Iter 2225/5000 - Score: 18.752\n",
      "Iter 2250/5000 - Loss: 2.408 - Lengthscale: 0.014 - Noise: 1.837 - Outputscale: 27.508\n",
      "Iter 2250/5000 - Score: 18.881\n",
      "Iter 2275/5000 - Loss: 2.407 - Lengthscale: 0.014 - Noise: 1.816 - Outputscale: 27.727\n",
      "Iter 2275/5000 - Score: 18.855\n",
      "Iter 2300/5000 - Loss: 2.407 - Lengthscale: 0.014 - Noise: 1.794 - Outputscale: 27.946\n",
      "Iter 2300/5000 - Score: 19.093\n",
      "Iter 2325/5000 - Loss: 2.398 - Lengthscale: 0.014 - Noise: 1.773 - Outputscale: 28.167\n",
      "Iter 2325/5000 - Score: 19.035\n",
      "Iter 2350/5000 - Loss: 2.401 - Lengthscale: 0.013 - Noise: 1.751 - Outputscale: 28.388\n",
      "Iter 2350/5000 - Score: 18.453\n",
      "Iter 2375/5000 - Loss: 2.406 - Lengthscale: 0.013 - Noise: 1.729 - Outputscale: 28.610\n",
      "Iter 2375/5000 - Score: 18.477\n",
      "Iter 2400/5000 - Loss: 2.397 - Lengthscale: 0.013 - Noise: 1.707 - Outputscale: 28.833\n",
      "Iter 2400/5000 - Score: 17.752\n",
      "Best model saved with score: 17.752\n",
      "Iter 2425/5000 - Loss: 2.400 - Lengthscale: 0.013 - Noise: 1.686 - Outputscale: 29.056\n",
      "Iter 2425/5000 - Score: 18.532\n",
      "Iter 2450/5000 - Loss: 2.398 - Lengthscale: 0.013 - Noise: 1.664 - Outputscale: 29.280\n",
      "Iter 2450/5000 - Score: 18.282\n",
      "Iter 2475/5000 - Loss: 2.393 - Lengthscale: 0.013 - Noise: 1.643 - Outputscale: 29.505\n",
      "Iter 2475/5000 - Score: 18.125\n",
      "Iter 2500/5000 - Loss: 2.395 - Lengthscale: 0.013 - Noise: 1.621 - Outputscale: 29.731\n",
      "Iter 2500/5000 - Score: 18.063\n",
      "Iter 2525/5000 - Loss: 2.394 - Lengthscale: 0.013 - Noise: 1.600 - Outputscale: 29.958\n",
      "Iter 2525/5000 - Score: 18.515\n",
      "Iter 2550/5000 - Loss: 2.393 - Lengthscale: 0.013 - Noise: 1.579 - Outputscale: 30.185\n",
      "Iter 2550/5000 - Score: 18.407\n",
      "Iter 2575/5000 - Loss: 2.389 - Lengthscale: 0.013 - Noise: 1.558 - Outputscale: 30.414\n",
      "Iter 2575/5000 - Score: 18.727\n",
      "Iter 2600/5000 - Loss: 2.395 - Lengthscale: 0.013 - Noise: 1.537 - Outputscale: 30.644\n",
      "Iter 2600/5000 - Score: 18.696\n",
      "Iter 2625/5000 - Loss: 2.395 - Lengthscale: 0.013 - Noise: 1.516 - Outputscale: 30.874\n",
      "Iter 2625/5000 - Score: 18.891\n",
      "Iter 2650/5000 - Loss: 2.397 - Lengthscale: 0.013 - Noise: 1.495 - Outputscale: 31.105\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "likelihood.train()\n",
    "model = model.to(device)\n",
    "\n",
    "training_iter = 5000\n",
    "best = 9999\n",
    "for i in range(training_iter):\n",
    "\n",
    "    # Zero backpropped gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Get output from model\n",
    "    output = model(x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, y)\n",
    "    loss.backward()\n",
    "\n",
    "    if (best < 21 and i % 25 == 0) or (i % 100 == 0):\n",
    "        print('Iter %d/%d - Loss: %.3f - Lengthscale: %.3f - Noise: %.3f - Outputscale: %.3f' % (\n",
    "            i, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            model.likelihood.noise.item(),\n",
    "            model.covar_module.outputscale.item()\n",
    "        ))\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            # print(\"eval\")\n",
    "            torch.save(model.state_dict(), 'model_state_temp.pth')\n",
    "            model2.load_state_dict(torch.load('model_state_temp.pth'))\n",
    "            output = model2(testx.to('cpu'))\n",
    "            gp_mean = output.mean\n",
    "            gp_mean*=largest\n",
    "            gp_std = output.stddev\n",
    "            score = test(gp_mean,gp_std)\n",
    "            print('Iter %d/%d - Score: %.3f' % (i, training_iter, score))\n",
    "            if score < best:\n",
    "                best = score\n",
    "                torch.save(model.state_dict(), 'model_state_improved6.pth')\n",
    "                print('Best model saved with score: %.3f' % (best))\n",
    "\n",
    "\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "# torch.save(model.state_dict(), 'model_state_improved6.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set into eval mode\n",
    "# model = model.to('cpu')\n",
    "# # model.load_state_dict(torch.load('model_state.pth', map_location = torch.device('cpu')))\n",
    "# model.eval()\n",
    "# likelihood.eval()\n",
    "# tt = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "#     output:MultivariateNormal = model(tt)\n",
    "#     observed_pred = likelihood(output)\n",
    "#     gp_mean = observed_pred.mean.cpu().numpy()\n",
    "#     gp_std = np.sqrt(observed_pred.variance.cpu().detach().numpy())\n",
    "#     print(output.mean.numpy())\n",
    "#     print(gp_mean)\n",
    "#     print(output.stddev.numpy())\n",
    "#     print(gp_std)\n",
    "\n",
    "# for mult in np.arange(0.32, 0.5, 0.02):\n",
    "#     for tresh in np.arange(1.01, 1.10, 0.01):\n",
    "#         predictions = gp_mean+gp_std*mult\n",
    "#         for i in range(gp_mean.size):\n",
    "#             tresh = tresh\n",
    "#             if predictions[i]>tresh*gp_mean[i]:\n",
    "#                 predictions[i] = tresh*gp_mean[i]\n",
    "\n",
    "#         print(\"Mult: \", mult, \"Tresh: \", tresh, \"Cost: \", cost_function(y_test, predictions))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
